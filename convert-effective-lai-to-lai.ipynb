{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert effective LAI to LAI\n",
    "## Overview\n",
    "This notebook demonstrates the conversion from effective to true leaf area index (LAI) using a clumping factor. It includes the propagation of the uncertainty to the true LAI, including many but not all additional sources of uncertainty.\n",
    "## Introduction\n",
    "The retrieval of LAI from optical sensors has to make assumptions about the canopy structure, which modifies the signal retrieved by the satellite. Different distributions (clumping) of the same amount of one-sided leaf area per unit ground (the definition of LAI) leads to different optical signals. Some products use a-priori information about the biome type, others, like TIP (used for C3S LAI v2 -- v4 or higher), assume a turbid medium. This is called an effective LAI. Arguably, depending on the degree of realism of the modelled canopy structure, there are differing degrees of effectiveness. The clumping parameter $\\Omega$ (sometimes $\\zeta$) relates true LAI to effective LAI as (cf. [Pinty et al. 2006](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005JD005952) or [Fang et al. 2019](https://doi.org/10.1029/2018RG000608)):\n",
    "\\begin{equation}\n",
    "\\textit{LAI}_\\textit{eff}(\\theta)=\\Omega(\\theta) \\times \\textit{LAI}.\n",
    "\\end{equation}\n",
    "The dependence on the solar zenith angle $\\theta$ is included here only for completeness. C3S-TIP LAI is computed for diffuse isotropic illumination.\n",
    "\n",
    "Depending on the application, it may or may not be meaningful to convert effective LAI to true LAI. Here, we use the Land Cover Class (LCC) - specific clumping indices found by [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) (also available [here](http://faculty.geog.utoronto.ca/Chen/Chen's%20homepage/PDFfiles2/RSE-Chen2005.pdf)) over a period of 8 months (Nov. 1996 -- June 1997) together with the C3S LCC product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "We use `xarray` for handling netCDF data sets and `numpy` for the computations. `OrderedDict` is required to acces dictionaries by index, and `datetime` for monitoring the performance of the individual cells. Enabling `dask` would be required for larger datasets, but it proved problematic to use a function of a dask array as index for the pre-computed factors in function `convert_LAI`. This may however be solvable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask # to be done\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    " - effective LAI, flags, and uncertainty from the CDS (Copernicus Data Store)\n",
    " - C3S LCC from the CDS\n",
    " - LCC-specific clumping factors from Table 3 of [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology<a id='Methodology'></a>\n",
    "The computation of the true LAI from effective LAI by dividing it with the clumping index may seem trivial, but knowing the correct clumping index is not easy. It actually introduces a number of uncertainties, as there are:\n",
    "\n",
    "  1. uncertainty of the land cover classification\n",
    "  2. clumping variation within same land cover class (different plant functional types, growth states &c.)\n",
    "  3. seasonal variation of clumping\n",
    "  4. mapping uncertainty between land cover classes\n",
    "\n",
    "(1) is addressed by using the confusion matrix of the C3S LCC product ([C3S LCC PQAR](https://datastore.copernicus-climate.eu/documents/satellite-land-cover/WP2-FDDP-LC-2021-2022-SENTINEL3-300m-v2.1.1_PQAR_v1.2_final.pdf)), also averaging (with probability weights) over the classes. This does, however, not account for temporal LCC changes which have not yet found their way into the LCC product.\n",
    "\n",
    "(2) is partly addressed by converting the range of Chen et al.'s clumping indices into an uncertainty by assuming that they define the 97.7%-quantiles, corresponding to 2 sigma of a Gaussian.\n",
    "\n",
    "(3) is neglected here, as well as the reported tendencies. Also note, that the clumping parameters of [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) were not computed over a full year.\n",
    "\n",
    "(4) is caused by the somewhat differing classification schemes used by [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) and C3S LCC. It is partly accounted for by averaging over the corresponding clumping indices where multiple 'Chen'-classes are assigned to one C3S LCC.\n",
    "\n",
    "Therefore, the presented methodology and its results should be interpreted with caution, as it merely demonstrates an approach to investigate the magnitude of the difference and the effects of the sources of uncertainty. A clumping index derived on the basis of plant functional types, including a phenological cycle is expected to give superior results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping between land cover classes\n",
    "We have implemented the land cover classes system (LCCS, cf. Appendix A of [C3S LCC PUG](https://datastore.copernicus-climate.eu/documents/satellite-land-cover/WP2-FDDP-LC-2021-2022-SENTINEL3-300m-v2.1.1_PUGS_v1.1_final.pdf)) and its mapping to the class numbers used in Table 3 of [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) as ordered dictionaries. This mapping is not authoritative and open to revision by the informed reader. We put all the Chen classes and related variables into a python class named `Clumping` to achieve a name space separation. This class also contains an uncertainty estimate of the Clumping index, derived from the range given in Chen et al.'s table in the way explained in the [Methodology section](#Methodology) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCCS_codes = [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220 ]\n",
    "\n",
    "LCCS_legend = OrderedDict(\n",
    "    {\n",
    "        0   : \"No Data\",\n",
    "        10  : \"Cropland, rainfed\",\n",
    "        20  : \"Cropland, irrigated or post-flooding\",\n",
    "        30  : \"Mosaic cropland (>50%) / natural vegetation (tree, shrub, herbaceous cover) (<50%)\",\n",
    "        40  : \"Mosaic natural vegetation (tree, shrub, herbaceous cover) (>50%) / cropland (<50%)\",\n",
    "        50  : \"Tree cover, broadleaved, evergreen, closed to open (>15%)\",\n",
    "        60  : \"Tree cover, broadleaved, deciduous, closed to open (>15%)\",\n",
    "        70  : \"Tree cover, needleleaved, evergreen, closed to open (>15%)\",\n",
    "        80  : \"Tree cover, needleleaved, deciduous, closed to open (>15%)\",\n",
    "        90  : \"Tree cover, mixed leaf type (broadleaved and needleleaved)\",\n",
    "        100 : \"Mosaic tree and shrub (>50%) / herbaceous cover (<50%)\",\n",
    "        110 : \"Mosaic herbaceous cover (>50%) / tree and shrub (<50%)\",\n",
    "        120 : \"Shrubland\",\n",
    "        130 : \"Grassland\",\n",
    "        140 : \"Lichens and mosses\",\n",
    "        150 : \"Sparse vegetation (tree, shrub, herbaceous cover) (<15%)\",\n",
    "        160 : \"Tree cover, flooded, fresh or brackish water\",\n",
    "        170 : \"Tree cover, flooded, saline water\",\n",
    "        180 : \"Shrub or herbaceous cover, flooded, fresh/saline/brackish water\",\n",
    "        190 : \"Urban areas\",\n",
    "        200 : \"Bare areas\",\n",
    "        210 : \"Water bodies\",\n",
    "        220 : \"Permanent snow and ice\"\n",
    "    }\n",
    ")\n",
    "\n",
    "class Clumping:\n",
    "#> Table 3 of Chen et al. (2005); https://doi.org/10.1016/j.rse.2005.05.003\n",
    "#> The land cover classification does not fully match the FAO Land Cover\n",
    "#> Classification System (LCCS).\n",
    "    \n",
    "    class_codes = [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "    legend = OrderedDict(\n",
    "        {\n",
    "            \"1\"  : \"Tree Cover, broadleaf, evergreen\",\n",
    "            \"2\"  : \"Tree Cover, broadleaf, deciduous, closed\",\n",
    "            \"3\"  : \"Tree Cover, broadleaf, deciduous, open\",\n",
    "            \"4\"  : \"Tree Cover, needleleaf, evergreen\",\n",
    "            \"5\"  : \"Tree Cover, needleleaf, deciduous\",\n",
    "            \"6\"  : \"Tree Cover, mixed leaf type\",\n",
    "            \"7\"  : \"Tree Cover, regularly flooded, fresh water\",\n",
    "            \"8\"  : \"Tree Cover, regularly flooded, saline water\",\n",
    "            \"9\"  : \"Mosaic: Tree Cover / Other natural vegetation\",\n",
    "            \"10\" : \"Tree Cover, burnt\",\n",
    "            \"11\" : \"Shrub Cover, closed-open, evergreen\",\n",
    "            \"12\" : \"Shrub Cover, closed-open, deciduous\",\n",
    "            \"13\" : \"Herbaceous Cover, closed-open\",\n",
    "            \"14\" : \"Sparse herbaceous or sparse shrub cover\",\n",
    "            \"15\" : \"Reg. flooded shrub and/or herbaceous cover\",\n",
    "            \"16\" : \"Cultivated and managed areas\",\n",
    "            \"17\" : \"Mosaic: Cropland / Tree Cover / Natural veg\",\n",
    "            \"18\" : \"Mosaic: Cropland / Shrub and/or grass cover\",\n",
    "            \"19\" : \"Bare Areas\" }\n",
    "    )\n",
    "\n",
    "#> This is the mapping from the LCCS to the classes used in Chen et al.; this\n",
    "#> may require adjustment! E.g. a grassland class seems to be missing.\n",
    "    class_of_LCCS = OrderedDict( # not mapped to: 10; unsure: 18, 19\n",
    "        {\n",
    "            10 : [16],\n",
    "            20 : [15,16],\n",
    "            30 : [17,18],\n",
    "            40 : [9],\n",
    "            50 : [1],\n",
    "            60 : [2,3],\n",
    "            70 : [4],\n",
    "            80 : [5],\n",
    "            90 : [6],\n",
    "            100 : [9],\n",
    "            110 : [9,13],\n",
    "# Chen et al.: \"The 'Shrub' and 'Grassland' classes were retained, as the modeling results for \n",
    "#               broadleaf trees can be extended to these classes [...]\"\n",
    "            120 : [1,11,12],\n",
    "            130 : [1], \n",
    "            140 : [19],\n",
    "            150 : [14],\n",
    "            160 : [7],\n",
    "            170 : [8],\n",
    "            180 : [15],\n",
    "            190 : [19],\n",
    "            200 : [19],\n",
    "            210 : [19],\n",
    "            220 : [19]\n",
    "        }\n",
    "    )\n",
    "# clumping index min, max, and mean from Chen et al.'s table:\n",
    "    ci_min = np.array([0.59,\t0.59,\t0.62,\t0.55,\t0.60,\t0.58,\t0.61,\n",
    "                       0.65,\t0.64,\t0.65,\t0.62,\t0.62,\t0.64,\t0.67,\n",
    "                       0.68,\t0.63,\t0.64,\t0.65,\t0.75])\n",
    "    \n",
    "    ci_max = np.array([0.68,\t0.79,\t0.78,\t0.68,\t0.77,\t0.79,\t0.69,\n",
    "                       0.79,\t0.82,\t0.86,\t0.80,\t0.80,\t0.83,\t0.84,\n",
    "                       0.85,\t0.83,\t0.76,\t0.81,\t0.99])\n",
    "\n",
    "    ci_mean = np.array([0.63,\t0.69,\t0.70,\t0.62,\t0.68,\t0.69,\t0.65,\n",
    "                        0.72,\t0.72,\t0.75,\t0.71,\t0.71,\t0.74,\t0.75,\n",
    "                        0.77,\t0.73,\t0.70,\t0.73,\t0.87])\n",
    "\n",
    "# For the uncertainty, we assume that the distribution is uniform above\n",
    "# and below the mean, respectively, thus making the mean also the\n",
    "# median. We then assume that the reported max and min values are the 97.7%-quantiles \n",
    "# which correspond to 2 sigma of a Gaussian distribution. The average over both sided is taken, to\n",
    "# account for cases where the values are not symmetric. Since no information\n",
    "# about the distribution is available in Chen et al. (2005), the clumping\n",
    "# index uncertainty is modelled as a Gaussian here. \n",
    "    ci_uncertainty = 0.5 * 0.5 * (ci_max - ci_min) # 1-sigma; ((max-mean)+(mean-min)=(max-min)\n",
    "# tendencies from Table 3, not used:\n",
    "    ci_d_NL = np.array([ -0.006 , -0.019,  -0.005,  -0.012, -0.033 , -0.024, np.nan   ,  np.nan   ,  -0.013,  -0.036, -0.020,  -0.016,  -0.016,  -0.019,  -0.026,  -0.018,  -0.011,  -0.018,  -0.032 ])\n",
    "    ci_d_EQ = np.array([ -0.004 , -0.001,  0.007 ,  -0.017, np.nan    , -0.018, -0.002,  -0.006,  0.008 ,  np.nan   , -0.010,  0.009 ,  0.003 ,  0.008 ,  0.004 ,  -0.006,  -0.004,  0.001 ,  -0.03  ])\n",
    "    ci_d_SL = np.array([ 0.024  , 0.021 ,  0.025 ,  0.009 , np.nan    , 0.011 , np.nan   ,  np.nan   ,  np.nan   ,  np.nan   , 0.024 ,  0.022 ,  0.026 ,  0.024 ,  0.024 ,  0.026 ,  0.024 ,  0.026 ,  0.027  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide the confusion matrix, copied from the [C3S LCC PQAR](https://datastore.copernicus-climate.eu/documents/satellite-land-cover/WP2-FDDP-LC-2021-2022-SENTINEL3-300m-v2.1.1_PQAR_v1.2_final.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusinon matrices of LCC, from C3S ICDR Land Cover Product Quality\n",
    "# Assessment Report (D5.2.2_PQAR_ICDR_LC_v2.1.x_PRODUCTS_v1.0)\n",
    "cm_2016 = np.array([\n",
    "    [ 121,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0  ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 9,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t3,\t7,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 3,\t0,\t0,\t0,\t199,\t15,\t3,\t0,\t3,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t72,\t1,\t8,\t16,\t0,\t0,\t13,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t3,\t23,\t3,\t0,\t0,\t3,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 2,\t0,\t0,\t0,\t8,\t10,\t3,\t1,\t0,\t0,\t0,\t6,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0  ],\n",
    "    [ 8,\t0,\t0,\t0,\t7,\t19,\t0,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0  ],\n",
    "    [ 8,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t12,\t0,\t0,\t1,\t1,\t25,\t1,\t1  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t25,\t0,\t0,\t0,\t0,\t11,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0  ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t13,\t0,\t0,\t1,\t0,\t61,\t0,\t0  ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ]\n",
    "])\n",
    "\n",
    "cm_2017 = np.array([\n",
    "    [ 121,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 9,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t3,\t7,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t199,\t15,\t3,\t0,\t3,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t72,\t1,\t8,\t16,\t0,\t0,\t13,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t54,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t3,\t23,\t3,\t0,\t0,\t3,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t8,\t10,\t3,\t1,\t0,\t0,\t0,\t6,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t7,\t19,\t0,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 8,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t12,\t0,\t0,\t1,\t1,\t25,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t25,\t0,\t0,\t0,\t0,\t11,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t13,\t0,\t0,\t1,\t0,\t61,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n",
    "\n",
    "cm_2018 = np.array([\n",
    "    [ 119,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 9,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t3,\t7,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t197,\t14,\t3,\t0,\t3,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t71,\t1,\t8,\t16,\t0,\t0,\t13,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t3,\t23,\t3,\t0,\t0,\t2,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t8,\t10,\t3,\t1,\t0,\t0,\t0,\t6,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t7,\t19,\t0,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 8,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t14,\t0,\t0,\t1,\t1,\t25,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t24,\t0,\t0,\t0,\t0,\t12,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t12,\t0,\t0,\t1,\t0,\t60,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n",
    "\n",
    "cm_2019 = np.array([\n",
    "    [ 119,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t2,\t7,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t198,\t14,\t3,\t0,\t2,\t0,\t0,\t3,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t71,\t1,\t8,\t16,\t0,\t0,\t12,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t2,\t23,\t3,\t0,\t0,\t2,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t8,\t10,\t2,\t1,\t0,\t0,\t0,\t5,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 7,\t0,\t0,\t0,\t7,\t19,\t2,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t14,\t0,\t0,\t1,\t1,\t25,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t23,\t0,\t0,\t0,\t0,\t12,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t12,\t0,\t0,\t1,\t0,\t60,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n",
    "\n",
    "cm_2020 = np.array([\n",
    "    [ 119,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t2,\t7,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t197,\t14,\t3,\t0,\t2,\t0,\t0,\t3,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t71,\t1,\t8,\t16,\t0,\t0,\t12,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t2,\t23,\t3,\t0,\t0,\t2,\t2,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t8,\t10,\t2,\t1,\t0,\t0,\t0,\t5,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 7,\t0,\t0,\t0,\t7,\t19,\t2,\t1,\t0,\t0,\t0,\t105,\t22,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t14,\t0,\t0,\t1,\t1,\t26,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t23,\t0,\t0,\t0,\t0,\t12,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t12,\t0,\t0,\t0,\t0,\t59,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the entries of the confusion matrices from absolute values into probabilities, we define the following routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_to_prob(confusion_matrix_LCC):\n",
    "    #> convert absolute numbers into probabilities:\n",
    "    confusion_matrix_prob_LCC = np.empty([len(confusion_matrix_LCC),len(confusion_matrix_LCC)])\n",
    "    for irow in range(len(confusion_matrix_LCC)):\n",
    "        rowsum = np.sum( confusion_matrix_LCC[irow][:] )\n",
    "        for icol in range(len(confusion_matrix_LCC[irow])):\n",
    "            if ( rowsum > 0 ):\n",
    "                confusion_matrix_prob_LCC[irow][icol] = ( confusion_matrix_LCC[irow][icol] / rowsum )\n",
    "            else:\n",
    "                confusion_matrix_prob_LCC[irow][icol] = 0.\n",
    "        # debug:print(irow,np.sum(confusion_matrix_prob_LCC[irow]))\n",
    "    return confusion_matrix_prob_LCC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can pre-compute the factors required in the conversion and in the uncertainty propagation, since they are a time-independent function of the LCC. The LAIs of all possible 'Chen'-classes related to one LCC by the confusion matrix and by mapping ambiguity are averaged, weighted by probability, to get one most probable LAI per C3S LCC type:\n",
    "\\begin{equation}\n",
    "  \\textit{LAI}_i(\\textit{LAI}_\\textit{eff},\\textrm{LCC}) = \\frac{1}{n}\\sum_{i=1}^n p_i(\\textrm{LCC}) \\times \n",
    "  \\frac{ \\textit{LAI}_\\textit{eff} } { \\Omega_i(\\textrm{LCC}) },\n",
    "\\end{equation}\n",
    "where $i$ is an index running over all $n$ 'Chen'-classes confused with one C3S LCC.\n",
    "\n",
    "Since the factor $\\textit{LAI}_\\textit{eff}$ is constant in the sum, the sum can be pre-computed into a conversion factor:\n",
    "\\begin{equation}\n",
    "  f\\_LAI\\_cl(LCC) = \\frac{1}{n} \\sum_{i=1}^n \\frac{ p_i(\\textrm{LCC}) } { \\Omega_i(\\textrm{LCC}) }.\n",
    "\\end{equation}\n",
    "With this factor, the conversion becomes\n",
    "\\begin{equation}\n",
    "  \\textit{LAI}_i(\\textit{LAI}_\\textit{eff},\\textrm{LCC}) = f\\_LAI\\_cl(LCC) \\times \\textit{LAI}_\\textit{eff}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For the uncertainty propagation, this means:\n",
    "\\begin{equation}\n",
    "  \\sigma_{\\textit{LAI}}(\\textit{LAI}_\\textit{eff},\\textrm{LCC})^2 = \\left( \\sigma_{\\textit{LAI}_\\textit{eff}} \\times f\\_LAI\\_cl \\right)^2 +   \n",
    "  \\left( \\sigma_{\\Omega(\\textrm{LCC})} \\times \\textit{LAI}_\\textit{eff} \\times\n",
    "  ( -\\frac{1}{n})\\sum_{i=1}^n \\frac{ p_i(\\textrm{LCC}) } { \\Omega_i(\\textrm{LCC})^2 } \\right)^2.\n",
    "\\end{equation}\n",
    "We also pre-compute the LAI-independent part of the factor in the second term of the above equation as\n",
    "\\begin{equation}\n",
    "    f\\_LAI\\_unc2\\_cl(LCC) = \\left( \\sigma_{\\Omega(\\textrm{LCC})} \\times (-\\frac{1}{n})\\sum_{i=1}^n \\frac{ p_i(\\textrm{LCC}) } { \\Omega_i(\\textrm{LCC})^2 } \\right)^2.\n",
    "\\end{equation}\n",
    "In the computation of the uncertainty, we are treating the uncertainty of effective LAI and the uncertainty of the the clumping factor as independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCC_to_ix(LCC):\n",
    "    return(int)((LCC-10)/10)\n",
    "\n",
    "def vLCC_to_ix(LCC):\n",
    "    mLCC = np.asarray(LCC)\n",
    "    scalar_input = False\n",
    "    if mLCC.ndim == 0:\n",
    "        mLCC = mLCC[None]  # Makes x 1D\n",
    "        scalar_input = True\n",
    "    if scalar_input:\n",
    "        return np.squeeze((int)((mLCC-10)/10+0.5))\n",
    "    return ((LCC-10)/10).round().astype(dtype=np.int8)\n",
    "    \n",
    "   \n",
    "def pre_compute_factors(confusion_matrix_prob_LCC):\n",
    "    #\n",
    "    # Pre-computation of factors for LAI and uncertainty computation\n",
    "    #\n",
    "    # Accounts for three sources of uncertainty:\n",
    "    # (a) Uncertainty of the effective LAI\n",
    "    # (b) Uncertainty caused by mis-classification of Land Cover Class\n",
    "    # (c) Uncertainty of the clumping factors\n",
    "    # (d) Mapping ambiguity between C3S LCC and Chen et al.'s classes\n",
    "    #\n",
    "    # intermediate \"true\" LAI uncertainty factor from clumping uncertainty:\n",
    "    f_LAI_unc_cl = np.zeros([len(LCCS_codes),len(Clumping.class_codes)]) \n",
    "    f_LAI_unc2_cl = np.zeros([len(LCCS_codes)])\n",
    "    # intermediate \"true\" LAI factor:\n",
    "    f_LAI_cl = np.zeros([len(LCCS_codes)])\n",
    "    for iLCC in range(len(LCCS_codes)):\n",
    "        if iLCC != LCC_to_ix(LCCS_codes[iLCC]):\n",
    "            raise \"dask limitation requires functional dependence between LCC code and array index.\"       \n",
    "        # compute variance caused by potential mis-classification\n",
    "        for icol in range(len(confusion_matrix_prob_LCC[iLCC])):\n",
    "            if ( confusion_matrix_prob_LCC[iLCC][icol] > 0. ):\n",
    "                # loop over list of mapped clumping classes\n",
    "                class_list = Clumping.class_of_LCCS[LCCS_codes[icol]]\n",
    "                nclasses = len(class_list)\n",
    "                print(\"LCC\",LCCS_codes[iLCC],\"could be LCC\",\n",
    "                      LCCS_codes[icol],\"with propability\",int(1000*confusion_matrix_prob_LCC[iLCC][icol])/1000,\"which maps to Chen et al. class(es)\",\n",
    "                      class_list, \"(\",nclasses,\")\") \n",
    "                if ( nclasses <= 0 ):\n",
    "                    raise Exception(\"unmapped class\")\n",
    "                for ct in class_list: # mapping ambiguity between LCCS and Chen classes, no uncertainty assigned\n",
    "                    iclump = Clumping.class_codes.index(ct)\n",
    "                    # accumulate LAI factor:\n",
    "                    f_LAI_cl[iLCC] += ( confusion_matrix_prob_LCC[iLCC][icol] /\n",
    "                                      Clumping.ci_mean[iclump] ) / nclasses \n",
    "                    # accumulate uncertainty factor per clumping class; negative sign from derivative arbitrary (squared away in next step), but\n",
    "                    # would matter if uncertainty sources were treated as correlated (extra terms):\n",
    "                    f_LAI_unc_cl[iLCC,iclump] -=  (\n",
    "                        Clumping.ci_uncertainty[iclump] *\n",
    "                        confusion_matrix_prob_LCC[iLCC][icol] /\n",
    "                        Clumping.ci_mean[iclump]**2 ) /  nclasses\n",
    "        f_LAI_unc2_cl[iLCC] = np.sum( f_LAI_unc_cl[iLCC,:]**2 )\n",
    "    return f_LAI_cl,f_LAI_unc2_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the actual computation, we can then make use of these factors, limiting the number of operations per pixel to the required minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_LAI(LAI_eff, LAI_eff_uncertainty,LCC_type):\n",
    "    iLCC = vLCC_to_ix(LCC_type-10)\n",
    "    LAI = f_LAI_cl[iLCC] * LAI_eff\n",
    "    LAI_uncertainty = np.sqrt( ( f_LAI_unc2_cl[iLCC] * LAI_eff**2 ) +\n",
    "                               ( f_LAI_cl[iLCC] * LAI_eff_uncertainty )**2 )\n",
    "    LAI_uncertainty = LAI_uncertainty.rename(LAI_eff_uncertainty.name)\n",
    "    LAI_uncertainty.attrs['units'] = LAI_eff_uncertainty.attrs['units'] # forward unit\n",
    "    return LAI.where(np.isfinite(LAI_eff)),LAI_uncertainty.where(np.isfinite(LAI_eff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up\n",
    "To actually run the conversion, data needs to be read, the pre-computation steps need to be called, and the output needs to be prepared. Here, we start with the set-up by providing the confusion matrices for all years. They are quite similar, and because there is only a limited number of cases, the sum of them all is thought to give a more robust statistics for the estimation of the confusion probabilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCC 10 could be LCC 10 with propability 0.681 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 10 could be LCC 20 with propability 0.17 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 10 could be LCC 50 with propability 0.011 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 10 could be LCC 60 with propability 0.005 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 10 could be LCC 120 with propability 0.028 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 10 could be LCC 130 with propability 0.079 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 10 could be LCC 150 with propability 0.011 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 10 could be LCC 180 with propability 0.005 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 10 could be LCC 200 with propability 0.005 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 20 could be LCC 10 with propability 0.257 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 20 could be LCC 20 with propability 0.685 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 20 could be LCC 60 with propability 0.057 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 30 could be LCC 10 with propability 0.462 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 30 could be LCC 50 with propability 0.215 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 30 could be LCC 60 with propability 0.053 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 30 could be LCC 120 with propability 0.107 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 30 could be LCC 130 with propability 0.161 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 40 could be LCC 10 with propability 0.307 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 40 could be LCC 20 with propability 0.038 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 40 could be LCC 60 with propability 0.269 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 40 could be LCC 120 with propability 0.1 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 40 could be LCC 130 with propability 0.269 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 40 could be LCC 150 with propability 0.015 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 50 could be LCC 10 with propability 0.013 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 50 could be LCC 50 with propability 0.882 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 50 could be LCC 60 with propability 0.064 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 50 could be LCC 70 with propability 0.013 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 50 could be LCC 90 with propability 0.011 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 50 could be LCC 120 with propability 0.01 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 50 could be LCC 130 with propability 0.004 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 60 could be LCC 10 with propability 0.008 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 60 could be LCC 50 with propability 0.051 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 60 could be LCC 60 with propability 0.615 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 60 could be LCC 70 with propability 0.008 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 60 could be LCC 80 with propability 0.068 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 60 could be LCC 90 with propability 0.137 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 60 could be LCC 120 with propability 0.108 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 70 could be LCC 50 with propability 0.11 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 70 could be LCC 60 with propability 0.033 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 70 could be LCC 70 with propability 0.589 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 70 could be LCC 80 with propability 0.022 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 70 could be LCC 90 with propability 0.177 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 70 could be LCC 120 with propability 0.022 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 70 could be LCC 150 with propability 0.044 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 80 could be LCC 70 with propability 0.068 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 80 could be LCC 80 with propability 0.608 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 80 could be LCC 90 with propability 0.079 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 80 could be LCC 120 with propability 0.063 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 80 could be LCC 130 with propability 0.074 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 80 could be LCC 150 with propability 0.105 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 90 could be LCC 60 with propability 0.111 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 90 could be LCC 70 with propability 0.055 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 90 could be LCC 80 with propability 0.055 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 90 could be LCC 90 with propability 0.777 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 100 could be LCC 10 with propability 0.063 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 100 could be LCC 50 with propability 0.212 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 100 could be LCC 60 with propability 0.265 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 100 could be LCC 70 with propability 0.069 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 100 could be LCC 80 with propability 0.026 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 100 could be LCC 120 with propability 0.148 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 100 could be LCC 130 with propability 0.132 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 100 could be LCC 150 with propability 0.053 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 100 could be LCC 180 with propability 0.026 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 110 could be LCC 70 with propability 0.096 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 110 could be LCC 120 with propability 0.258 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 110 could be LCC 130 with propability 0.483 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 110 could be LCC 180 with propability 0.161 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 120 could be LCC 10 with propability 0.044 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 120 could be LCC 50 with propability 0.041 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 120 could be LCC 60 with propability 0.111 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 120 could be LCC 70 with propability 0.004 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 120 could be LCC 80 with propability 0.005 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 120 could be LCC 120 with propability 0.615 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 120 could be LCC 130 with propability 0.124 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 120 could be LCC 150 with propability 0.046 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 120 could be LCC 200 with propability 0.005 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 10 with propability 0.06 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 130 could be LCC 20 with propability 0.021 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 130 could be LCC 90 with propability 0.007 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 130 could be LCC 120 with propability 0.136 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 130 could be LCC 130 with propability 0.461 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 130 could be LCC 140 with propability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 150 with propability 0.095 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 130 could be LCC 180 with propability 0.007 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 130 could be LCC 190 with propability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 200 with propability 0.181 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 210 with propability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 220 with propability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 140 could be LCC 140 with propability 0.666 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 140 could be LCC 150 with propability 0.333 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 150 could be LCC 10 with propability 0.056 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 150 could be LCC 120 with propability 0.259 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 150 could be LCC 130 with propability 0.248 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 150 could be LCC 140 with propability 0.033 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 150 could be LCC 150 with propability 0.27 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 150 could be LCC 200 with propability 0.13 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 160 could be LCC 50 with propability 1.0 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 170 could be LCC 180 with propability 1.0 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 180 could be LCC 130 with propability 0.5 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 180 could be LCC 150 with propability 0.083 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 180 could be LCC 160 with propability 0.083 which maps to Chen et al. class(es) [7] ( 1 )\n",
      "LCC 180 could be LCC 180 with propability 0.333 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 190 could be LCC 60 with propability 0.25 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 190 could be LCC 190 with propability 0.75 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 200 could be LCC 10 with propability 0.025 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 200 could be LCC 120 with propability 0.012 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 200 could be LCC 130 with propability 0.037 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 200 could be LCC 150 with propability 0.156 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 200 could be LCC 180 with propability 0.01 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 200 could be LCC 200 with propability 0.758 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 210 could be LCC 20 with propability 0.016 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 210 could be LCC 90 with propability 0.016 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 210 could be LCC 180 with propability 0.003 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 210 could be LCC 210 with propability 0.962 which maps to Chen et al. class(es) [19] ( 1 )\n"
     ]
    }
   ],
   "source": [
    "# set-up\n",
    "confusion_matrix_prob_LCC = abs_to_prob(cm_2016+cm_2017+cm_2018+cm_2019+cm_2020)\n",
    "f_LAI_cl,f_LAI_unc2_cl = pre_compute_factors(confusion_matrix_prob_LCC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something to play around with\n",
    "Feel free to vary the inputs in the following cell, as long as all arrays have the same length. This should give you an idea of the magnitude of change of LAI and its uncertainties caused by the conversion for different LCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCC type <xarray.DataArray 'LCCS_type' (dim_0: 6)>\n",
      "array([160, 160, 160, 120, 120, 120])\n",
      "Dimensions without coordinates: dim_0\n",
      "<xarray.DataArray 'LCCS_type' ()>\n",
      "array(160)  :  Tree cover, flooded, fresh or brackish water\n",
      "<xarray.DataArray 'LCCS_type' ()>\n",
      "array(160)  :  Tree cover, flooded, fresh or brackish water\n",
      "<xarray.DataArray 'LCCS_type' ()>\n",
      "array(160)  :  Tree cover, flooded, fresh or brackish water\n",
      "<xarray.DataArray 'LCCS_type' ()>\n",
      "array(120)  :  Shrubland\n",
      "<xarray.DataArray 'LCCS_type' ()>\n",
      "array(120)  :  Shrubland\n",
      "<xarray.DataArray 'LCCS_type' ()>\n",
      "array(120)  :  Shrubland\n",
      "LAI_eff : <xarray.DataArray 'LAI' (dim_0: 6)>\n",
      "array([1., 2., 3., 1., 2., 3.])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2 \tLAI_eff_unc :  <xarray.DataArray 'LAI_ERR' (dim_0: 6)>\n",
      "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n",
      "LAI     : <xarray.DataArray 'LAI' (dim_0: 6)>\n",
      "array([1.40312771, 2.80625543, 4.20938314, 1.51246069, 3.02492137,\n",
      "       4.53738206])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2 \tLAI_unc     :  <xarray.DataArray 'LAI_ERR' (dim_0: 6)>\n",
      "array([0.28257368, 0.28833913, 0.29770019, 0.30473558, 0.31136893,\n",
      "       0.3221211 ])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "LAI = np.empty([6])\n",
    "LAI_uncertainty = np.empty([6])\n",
    "# input\n",
    "LAI_eff = xr.DataArray([1.,2.,3.,1.,2.,3.],name='LAI',attrs={'units':'m2.m-2'})\n",
    "LAI_eff_uncertainty = xr.DataArray([0.2,0.2,0.2,0.2,0.2,0.2],name='LAI_ERR',attrs={'units':'m2.m-2'})\n",
    "LCC_type = xr.DataArray([160,160,160, 120, 120, 120],name='LCCS_type')\n",
    "# actual conversion:\n",
    "LAI, LAI_uncertainty = convert_LAI(LAI_eff, LAI_eff_uncertainty, LCC_type)\n",
    "print(\"LCC type\",LCC_type)\n",
    "for cLCC in LCC_type:\n",
    "    print(cLCC,\" : \",LCCS_legend[(int)(cLCC.compute())])\n",
    "print(\"LAI_eff :\",LAI_eff,\"\\tLAI_eff_unc : \",LAI_eff_uncertainty)\n",
    "print(\"LAI     :\",LAI    ,\"\\tLAI_unc     : \",LAI_uncertainty    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting the Data: The Climate Data Store (CDS)\n",
    "\n",
    "We will be using the CDS API. Its usage is explained on the [CDS API how-to pages](https://cds.climate.copernicus.eu/how-to-api). Please consult the tutorial on how to set up a CDS API key, which will be prerequisite to the following code to work. Our code defines a python subroutine `get_data` which downloads LAI and fAPAR data as gzip'ped `tar` archive from the CDS and writes it to the file given by the `target` argument of `get_data`.\n",
    "The data is retained in a file to avoid repeated downloading when the notebook is run repeatedly and to avoid holding the whole data in memory. This commes at the cost of dublicationg the data on disk in the subsequent extraction step. Currently, there seems to be no way to avoid this, because the CDS /always/ delivers compressed archives when the data is ordered as file.\n",
    "\n",
    "For this tutorial, we are downloading global effective LAI V4 based on Sentinel-3 (OLCI and SLSTR) data with a resolution of 300 km for April 10, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:30:51,593 INFO Request ID is 296a5374-5ac4-4c63-b5f2-3597e68d9636\n",
      "2024-12-05 11:30:51,660 INFO status has been updated to accepted\n",
      "2024-12-05 11:30:57,725 INFO status has been updated to running\n",
      "2024-12-05 12:25:49,107 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c937f502b98f0b3665f9e36de63574c0.gz:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got LAI data,       elapsed: 0:55:17.027275\n"
     ]
    }
   ],
   "source": [
    "def get_data(dataset, request, target):\n",
    "    import cdsapi\n",
    "    import os.path\n",
    "    if os.path.isfile(target):\n",
    "        print(\"target\",target,\"already exists.\")\n",
    "    else:\n",
    "        client = cdsapi.Client()\n",
    "        client.retrieve(dataset,request,target) #.download()\n",
    "            \n",
    "starttime = datetime.now()\n",
    "dataset = 'satellite-lai-fapar'\n",
    "request = {\n",
    "    'format': 'tgz',\n",
    "    'variable': ['lai'],\n",
    "    'satellite': ['sentinel_3'],\n",
    "    'sensor': 'olci_and_slstr',\n",
    "    'horizontal_resolution': ['300m'],\n",
    "    'product_version': 'v4', # the difference to the old CDS is here\n",
    "    'year': ['2019'],\n",
    "    'month': ['04'],\n",
    "    'nominal_day': ['10'],\n",
    "    'area': [60, 0, 40, 20]\n",
    "}\n",
    "\n",
    "laifile = 'laidata.tgz'\n",
    "get_data(dataset,request,target=laifile)\n",
    "print('got LAI data,       elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar step is required for the LCC data, to get the corresponding land cover classes for 2019. The resolution of this product is 300m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:25:59,962 INFO Request ID is 1e8083a2-611f-4551-9d24-62666cf0d8ce\n",
      "2024-12-05 12:26:00,972 INFO status has been updated to accepted\n",
      "2024-12-05 12:26:05,349 INFO status has been updated to running\n",
      "2024-12-05 12:34:25,009 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "651a66809a84c81a26d24cd7a9116bb5.gz:   0%|          | 0.00/51.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got LCC data,       elapsed: 0:08:31.194931\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "dataset = \"satellite-land-cover\"\n",
    "request = {\n",
    "    'format': 'tgz',\n",
    "    'variable': 'all',\n",
    "    'year': ['2019'],\n",
    "    'version': ['v2_1_1'], # the difference to the old CDS is here\n",
    "    'area': [60, 0, 40, 20]\n",
    "}\n",
    "lccfile = 'lccdata.tgz'\n",
    "get_data(dataset,request,target=lccfile)\n",
    "print('got LCC data,       elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened tar file,  elapsed: 0:00:00.001765\n",
      "?rw-r--r-- root/root  112364294 2024-12-05 12:25:36 c3s_LAI_20190410000000_GLOBE_SENTINEL3_V4.0.1.area-subset.60.0.40.20.nc \n",
      "listing,     elapsed: 0:00:00.236564\n",
      "extracting  c3s_LAI_20190410000000_GLOBE_SENTINEL3_V4.0.1.area-subset.60.0.40.20.nc\n",
      "opened tar file,  elapsed: 0:00:00.590531\n",
      "?rw-r--r-- root/root   57447299 2024-12-05 12:33:35 C3S-LC-L4-LCCS-Map-300m-P1Y-2019-v2.1.1.area-subset.60.0.40.20.nc \n",
      "listing,     elapsed: 0:00:00.693927\n",
      "extracting  C3S-LC-L4-LCCS-Map-300m-P1Y-2019-v2.1.1.area-subset.60.0.40.20.nc\n",
      "unpacked data,    elapsed: 0:00:00.859791\n"
     ]
    }
   ],
   "source": [
    "def unpack_data(file):\n",
    "    import tarfile\n",
    "    import os.path\n",
    "    tf = tarfile.open(name=file,mode='r')\n",
    "    print('opened tar file,  elapsed:',datetime.now()-starttime)\n",
    "    tf.list()\n",
    "    print('listing,     elapsed:',datetime.now()-starttime)\n",
    "    # just extract what is not present:\n",
    "    for xfile in tf:\n",
    "        if os.path.isfile(xfile.name) == False:\n",
    "            print('extracting ',xfile.name)\n",
    "            tf.extract(member=xfile.name,path='.') # uses current working directory        \n",
    "        else:\n",
    "            print('present    ',xfile.name)\n",
    "    return tf\n",
    "\n",
    "starttime = datetime.now()\n",
    "lai_tarfileinfo = unpack_data(laifile)\n",
    "lcc_tarfileinfo = unpack_data(lccfile)\n",
    "print('unpacked data,    elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "The data are prepared for reading by passing their names to an `xarray` multi-file object called `filedata` here. This will enable `dask` to work in parallel on multiple files and to hold only subsets in memory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up inout files: 0:00:07.404059\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "varname = 'LAI' \n",
    "uncname = varname + '_ERR' # name of uncertainty layer\n",
    "# extract the file names containting `varname` from `tarfileinfo`\n",
    "inputfiles = [] # start with empty list\n",
    "for xfile in lai_tarfileinfo:\n",
    "    if varname.casefold() in xfile.name.casefold():\n",
    "        inputfiles.append(xfile.name)\n",
    "# give the list to an `xarray` multi-file object\n",
    "#dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "#laifiledata = xr.open_mfdataset(inputfiles,chunks='auto',parallel=True)\n",
    "#laifiledata = xr.open_dataset(inputfiles[0],chunks='auto')\n",
    "laifiledata = xr.load_dataset(inputfiles[0])\n",
    "lccname = 'lccs_class'\n",
    "inputfiles = [] # start with empty list\n",
    "for xfile in lcc_tarfileinfo:\n",
    "    if 'LCCS-Map'.casefold() in xfile.name.casefold():\n",
    "        inputfiles.append(xfile.name)\n",
    "#lccfiledata = xr.open_mfdataset(inputfiles,chunks='auto',parallel=True)\n",
    "#lccfiledata = xr.open_dataset(inputfiles[0],chunks='auto')\n",
    "lccfiledata = xr.load_dataset(inputfiles[0])\n",
    "\n",
    "#\n",
    "print('set up inout files:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying quality flags\n",
    "TIP LAI and fAPAR come with a set of infomrational and quality flag, stored in the layer `retrival_flag`. We are using the hexadecimal representation `0x1C1` of the bit array `111000001`, here, to avoid cells with the conditions `obs_is_fillvalue`, `tip_untrusted`,`obs_unusable`, and `obs_inconsistent` (see [PUG](https://datastore.copernicus-climate.eu/documents/satellite-lai-fapar/D3.3.10-v4.1_PUGS_CDR-ICDR_LAI_FAPAR_SENTINEL3_v4.0_PRODUCTS_v1.1.pdf) for reference). In the end, we must not forget to define the units of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applied flags,  elapsed: 0:00:00.619215\n"
     ]
    }
   ],
   "source": [
    "def apply_flags(data,fielddict):\n",
    "    import numpy as np\n",
    "    func     = lambda val, flags : np.where( (np.bitwise_and(flags.astype('uint32'),0x1C1) == 0x0 ), val, np.nan )\n",
    "    units = data[fielddict['variable']].attrs['units']\n",
    "    clean_data = xr.apply_ufunc(func,\\\n",
    "                                data[fielddict['variable']],\\\n",
    "                                data[fielddict['flags']],\\\n",
    "                                dask=\"allowed\",dask_gufunc_kwargs={'allow_rechunk':True})\n",
    "    # set units of result:\n",
    "    clean_data.attrs['units'] = units\n",
    "    return clean_data\n",
    "\n",
    "starttime = datetime.now()\n",
    "laifiledata[varname] = apply_flags(laifiledata,{'variable':varname,'flags':'retrieval_flag'})\n",
    "laifiledata[uncname] = apply_flags(laifiledata,{'variable':uncname,'flags':'retrieval_flag'})\n",
    "print('applied flags,  elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the conversion\n",
    "It turns out that the datasets to be combined are not defined on the same grid, even if both use a global regular grid with approx. 300m resolution at the equator. Therefore, a nearest neighbour interpolation is nested into the call of the `conversion_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'LAI' (time: 1, lat: 6720, lon: 6720)>\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [0.75202197, 0.64169085, 0.6520677 , ..., 0.8435831 ,\n",
      "         0.9189684 , 0.55409735],\n",
      "        [0.6748054 , 0.61712193,        nan, ..., 0.8974515 ,\n",
      "         0.8507554 , 0.916069  ],\n",
      "        [0.6218526 , 0.58675414, 0.6010987 , ..., 0.8415993 ,\n",
      "         0.55287653, 0.61162823]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2019-04-10\n",
      "  * lon      (lon) float64 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n",
      "<xarray.DataArray 'LAI_ERR' (time: 1, lat: 6720, lon: 6720)>\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [0.61208606, 0.5263238 , 0.5374637 , ..., 0.84816116,\n",
      "         0.9334656 , 0.5267816 ],\n",
      "        [0.5716466 , 0.528155  ,        nan, ..., 0.9446055 ,\n",
      "         0.7863574 , 0.8448039 ],\n",
      "        [0.55272394, 0.51960933, 0.5835495 , ..., 0.7639249 ,\n",
      "         0.49641386, 0.441935  ]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2019-04-10\n",
      "  * lon      (lon) float64 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n",
      "<xarray.DataArray 'lccs_class' (time: 1, lat: 7200, lon: 7200)>\n",
      "array([[[210, 210, 210, ..., 210, 210, 210],\n",
      "        [210, 210, 210, ..., 210, 210, 210],\n",
      "        [210, 210, 210, ..., 210, 210, 210],\n",
      "        ...,\n",
      "        [ 12,  12,  12, ...,  12,  40,  40],\n",
      "        [ 12,  12,  12, ...,  12,  12,  30],\n",
      "        [ 12,  12,  12, ...,  12,  12,  12]]], dtype=uint8)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 60.0 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "  * lon      (lon) float64 0.001389 0.004167 0.006944 ... 19.99 20.0 20.0\n",
      "  * time     (time) datetime64[ns] 2019-04-10\n",
      "Attributes:\n",
      "    standard_name:        land_cover_lccs\n",
      "    flag_colors:          #ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #00...\n",
      "    long_name:            Land cover class defined in LCCS\n",
      "    valid_min:            1\n",
      "    valid_max:            220\n",
      "    ancillary_variables:  processed_flag current_pixel_state observation_coun...\n",
      "    flag_meanings:        no_data cropland_rainfed cropland_rainfed_herbaceou...\n",
      "    flag_values:          [  0  10  11  12  20  30  40  50  60  61  62  70  7...\n",
      "<xarray.DataArray 'lccs_class' (time: 1, lat: 6720, lon: 6720)>\n",
      "array([[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        [ nan, 210., 210., ..., 210., 210., 210.],\n",
      "        [ nan, 210., 210., ..., 210., 210.,  70.],\n",
      "        ...,\n",
      "        [ nan,  12.,  12., ...,  11.,  12.,  40.],\n",
      "        [ nan,  12.,  12., ...,  10.,  12.,  12.],\n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan]]])\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2019-04-10\n",
      "  * lon      (lon) float64 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    standard_name:        land_cover_lccs\n",
      "    flag_colors:          #ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #00...\n",
      "    long_name:            Land cover class defined in LCCS\n",
      "    valid_min:            1\n",
      "    valid_max:            220\n",
      "    ancillary_variables:  processed_flag current_pixel_state observation_coun...\n",
      "    flag_meanings:        no_data cropland_rainfed cropland_rainfed_herbaceou...\n",
      "    flag_values:          [  0  10  11  12  20  30  40  50  60  61  62  70  7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/.pyenv/versions/3.8.14/lib/python3.8/site-packages/xarray/core/duck_array_ops.py:187: RuntimeWarning: invalid value encountered in cast\n",
      "  return data.astype(dtype, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up computation,  elapsed: 0:00:08.189314\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "# Test output to check objects\n",
    "print (laifiledata[varname])\n",
    "print (laifiledata[uncname])\n",
    "lccfiledata['time'] = laifiledata['time'] # to align time dimension\n",
    "print(lccfiledata[lccname])\n",
    "print(lccfiledata[lccname].\\\n",
    "                         sel(lat=slice(laifiledata.lat.max(),laifiledata.lat.min())).\\\n",
    "                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=False))\n",
    "# define conversion as lambda function to apply in dask delayed computation\n",
    "conversion_func = lambda var, unc, lcc : convert_LAI(LAI_eff=var, LAI_eff_uncertainty=unc, LCC_type=lcc)\n",
    "# set up the conversion\n",
    "#LAI  = dask.array.apply_gufunc(conversion_func,\n",
    "#                      '(i,j),(i,j),(i,j)->(i,j)',\n",
    "#                      laifiledata[varname],\n",
    "#                      laifiledata[uncname],\n",
    "#                      lccfiledata[lccname].\\\n",
    "#                         sel(lat=slice(laifiledata.lat.min(),laifiledata.lat.max())).\\\n",
    "#                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=True)\n",
    "#                      )\n",
    "#LAI, LAIu  = xr.apply_ufunc(conversion_func,\n",
    "#                      laifiledata[varname],\n",
    "#                      laifiledata[uncname],\n",
    "#                      lccfiledata[lccname].\\\n",
    "#                         sel(lat=slice(laifiledata.lat.max(),laifiledata.lat.min())).\\\n",
    "#                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=False),\n",
    "#                      dask=\"allowed\")\n",
    "LAI = convert_LAI(LAI_eff = laifiledata[varname],\n",
    "                   LAI_eff_uncertainty = laifiledata[uncname],\n",
    "                   LCC_type = lccfiledata[lccname].\\\n",
    "                         sel(lat=slice(laifiledata.lat.max(),laifiledata.lat.min())).\\\n",
    "                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=False))\n",
    "\n",
    "print('set up computation,  elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be an \"*RuntimeWarning: invalid value encountered in cast*\" caused by the missing values in the input data, which seems safe to be ignored.\n",
    "\n",
    "With dask, the computation could actually be delayed until the data values are used, and large datasets could be process in parallel. Here, we run an output method, which would start the sliced computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<xarray.DataArray 'LAI' (time: 1, lat: 6720, lon: 6720)>\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [1.04131915, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.8122886 ],\n",
      "        [0.93439795, 0.        ,        nan, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.86107457, 0.81247404, 0.83233686, ..., 1.16535618,\n",
      "         0.76556396, 0.846917  ]]])\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2019-04-10\n",
      "  * lon      (lon) float64 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2, <xarray.DataArray 'LAI_ERR' (time: 1, lat: 6720, lon: 6720)>\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [0.84930186, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.77273374],\n",
      "        [0.79306436, 0.        ,        nan, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.76667871, 0.72075493, 0.80921031, ..., 1.05955842,\n",
      "         0.68854762, 0.61354773]]])\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2019-04-10\n",
      "  * lon      (lon) float64 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2)\n",
      "Writing to  LAI-clumped.nc\n",
      "After writing output : 0:00:12.572351\n"
     ]
    }
   ],
   "source": [
    "print(LAI)\n",
    "#\n",
    "# output to file triggers the delayed computation:\n",
    "#\n",
    "def write_result(output,outfile):\n",
    "    import os.path\n",
    "    if os.path.isfile(outfile):\n",
    "        print(\"file\",outfile,\"already exists. Writing skipped.\")\n",
    "    else:\n",
    "        print(\"Writing to \",outfile)\n",
    "        output.to_netcdf(outfile,mode='w',encoding={varname:{\"zlib\": True, \"complevel\": 4,},uncname:{\"zlib\": True, \"complevel\": 4}})\n",
    "    return\n",
    "\n",
    "starttime = datetime.now()\n",
    "output = xr.merge(LAI)\n",
    "outfile = varname + '-clumped.nc'\n",
    "write_result(output,outfile)\n",
    "output.close()\n",
    "print('After writing output :',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "The notebook leaves the following files behind:\n",
    "- *laidata.tgz* - compressed tar archive retrieved from CDS\n",
    "- *lccdata.tgz* - compressed tar archive retrieved from CDS\n",
    "- *c3s_LAI_20190410000000_GLOBE_SENTINEL3_V4.0.1.area-subset.60.0.40.20.nc* - input LAI data\n",
    "- *C3S-LC-L4-LCCS-Map-300m-P1Y-2019-v2.1.1.area-subset.60.0.40.20.nc* - input LCC data\n",
    "- *LAI-clumped.nc* - output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
