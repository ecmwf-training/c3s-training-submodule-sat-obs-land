{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "logo"
    ]
   },
   "source": [
    "![logo](https://climate.copernicus.eu/sites/default/files/2025-03/logoline_c3s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Working with Leaf Area Index available through CDS to convert effective LAI to LAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "run"
    ]
   },
   "source": [
    "**This notebook can be run on free online platforms, such as Binder, Kaggle and Colab, or they can be accessed from GitHub. The links to run this notebook in these environments are provided here, but please note they are not supported by ECMWF.**\n",
    "\n",
    "[![binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/ecmwf-training/c3s-training-submodule-sat-obs-land/develop?labpath=convert-effective-lai-to-lai.ipynb)\n",
    "[![kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/ecmwf-training/c3s-training-submodule-sat-obs-land/blob/develop/convert-effective-lai-to-lai.ipynb)\n",
    "[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ecmwf-training/c3s-training-submodule-sat-obs-land/blob/develop/convert-effective-lai-to-lai.ipynb)\n",
    "[![github](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](https://github.com/ecmwf-training/c3s-training-submodule-sat-obs-land/blob/develop/convert-effective-lai-to-lai.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "objectives"
    ]
   },
   "source": [
    "## Learning objectives ðŸŽ¯\n",
    "Leaf Area Index (LAI) is a measure of the total one-sided leaf surface area per unit ground area. It is known to be a proxy for surface aspects that directly influence energy exchange, carbon uptake, and water cycling in ecosystems,  playing a critical role in climate modeling and land surface monitoring. Without exact knowledge of the canopy structure this true LAI is hard to retrieve with optical remtote sensing, leading to the concept of effective LAI. This notebook demonstrates the conversion from effective to true LAI using a clumping factor. It includes the propagation of the uncertainty to the true LAI, including many but not all additional sources of uncertainty.\n",
    "\n",
    "In this Jupyter notebook, you will:\n",
    "- Understand the difference between effective and true Leaf Area Index, and why conversion may be required for your use case;\n",
    "- Learn how to set up and prepare data for a Leaf Area Index (LAI) conversion algorithm, and then execute the conversion process;\n",
    "- Access and utilize LAI-related data from the Climate Data Store (CDS);\n",
    "- Explore the complexities of LAI conversions and recognize the importance of careful interpretation of results;\n",
    "- Get an impression of the uncertainties involved in the computation of true LAI.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Methodology\n",
    "The retrieval of LAI from optical sensors has to make assumptions about the canopy structure, which modifies the signal retrieved by the satellite. Different distributions (clumping) of the same amount of one-sided leaf area per unit ground (the definition of LAI) leads to different optical signals. Some products use a-priori information about the biome type, others, like TIP (used for C3S LAI v2 -- v4 or higher), assume a turbid medium. This is called an effective LAI. Arguably, depending on the degree of realism of the modelled canopy structure, there are differing degrees of effectiveness. The clumping parameter $\\Omega$ (sometimes $\\zeta$) relates true LAI to effective LAI as (cf. [Pinty et al. 2006](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005JD005952) or [Fang et al. 2019](https://doi.org/10.1029/2018RG000608)):\n",
    "\\begin{equation}\n",
    "\\textit{LAI}_\\textit{eff}(\\theta)=\\Omega(\\theta) \\times \\textit{LAI}.\n",
    "\\end{equation}\n",
    "The dependence on the solar zenith angle $\\theta$ is included here only for completeness. C3S-TIP LAI is computed for diffuse isotropic illumination.\n",
    "\n",
    "Depending on the application, it may or may not be meaningful to convert effective LAI to true LAI. Here, we use the Land Cover Class (LCC) - specific clumping indices found by [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) (also available [here](http://faculty.geog.utoronto.ca/Chen/Chen's%20homepage/PDFfiles2/RSE-Chen2005.pdf)) over a period of 8 months (Nov. 1996 -- June 1997) together with the C3S LCC product.\n",
    "\n",
    "The computation of the true LAI from effective LAI by dividing it with the clumping index may seem trivial, but knowing the correct clumping index is not easy. It actually introduces a number of uncertainties, as there are:\n",
    "\n",
    "  1. uncertainty of the land cover classification\n",
    "  2. clumping variation within same land cover class (different plant functional types, growth states &c.)\n",
    "  3. seasonal variation of clumping\n",
    "  4. mapping uncertainty between land cover classes\n",
    "\n",
    "(1) is addressed by using the confusion matrix of the C3S LCC product ([C3S LCC v2.1 Product user guide](https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=documentation)), also averaging (with probability weights) over the classes. This does, however, not account for temporal LCC changes which have not yet found their way into the LCC product.\n",
    "\n",
    "(2) is partly addressed by converting the range of Chen et al.'s clumping indices into an uncertainty by assuming that they define the 97.7%-quantiles, corresponding to 2 sigma of a Gaussian.\n",
    "\n",
    "(3) is neglected here, as well as the reported tendencies. Also note, that the clumping parameters of [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) were not computed over a full year.\n",
    "\n",
    "(4) is caused by the somewhat differing classification schemes used by [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) and C3S LCC. It is partly accounted for by averaging over the corresponding clumping indices where multiple 'Chen'-classes are assigned to one C3S LCC.\n",
    "\n",
    "Therefore, the presented methodology and its results should be interpreted with caution, as it merely demonstrates an approach to investigate the magnitude of the difference and the effects of the sources of uncertainty. A clumping index derived on the basis of plant functional types, including a phenological cycle is expected to give superior results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "api"
    ]
   },
   "source": [
    "## Prepare your environment\n",
    "\n",
    "### Set up CDSAPI and your credentials\n",
    "\n",
    "The code below will ensure that the `cdsapi` package is installed. If you have not setup your `~/.cdsapirc` file with your credenials, you can replace `None` with your credentials that can be found on the [how to api](https://cds.climate.copernicus.eu/how-to-api) page (you will need to log in to see your credentials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "api"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -q cdsapi\n",
    "# If you have already setup your .cdsapirc file you can leave this as None\n",
    "cdsapi_key = None\n",
    "cdsapi_url = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "import"
    ]
   },
   "source": [
    "### (Install and) Import libraries\n",
    "We use `xarray` for handling netCDF data sets and `numpy` for the computations. `OrderedDict` is required to access dictionaries by index, and `datetime` for monitoring the performance of the individual cells. Enabling `dask` would be required for larger datasets, but it proved problematic to use a function of a dask array as index for the pre-computed factors in function `convert_LAI`. This may however be solvable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "import"
    ]
   },
   "outputs": [],
   "source": [
    "#import dask # to be done\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "request"
    ]
   },
   "source": [
    "## Explore data\n",
    "\n",
    "### Input data\n",
    " - [effective LAI, flags, and uncertainty](https://cds.climate.copernicus.eu/datasets/satellite-lai-fapar?tab=overview) from the CDS\n",
    " - [C3S LCC](https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=overview) from the CDS\n",
    " - LCC-specific clumping factors from Table 3 of [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Set things up\n",
    "To actually run the conversion, definitions for the algorithm need to be made, data needs to be read, the pre-computation steps need to be called, and the output needs to be prepared. Before any data is downloaded, you will have a chance to check these settings in section 'Aside: something to play around with' further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Correspondence between land cover class systems\n",
    "We have implemented the land cover classes system (LCCS, cf. Appendix A of [C3S LCC v2.1 Product user guide](https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=documentation)) and its mapping to the class numbers used in Table 3 of [Chen et al. (2005)](https://doi.org/10.1016/j.rse.2005.05.003) as ordered dictionaries. This mapping is not authoritative and open to revision by the informed reader. We put all the Chen classes and related variables into a python class named `Clumping` to achieve a name space separation. This class also contains an uncertainty estimate of the Clumping index, derived from the range given in Chen et al.'s table in the way explained in the [Methodology section](#Methodology) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LCCS_codes = [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220 ]\n",
    "\n",
    "LCCS_legend = OrderedDict(\n",
    "    {\n",
    "        0   : \"No Data\",\n",
    "        10  : \"Cropland, rainfed\",\n",
    "        20  : \"Cropland, irrigated or post-flooding\",\n",
    "        30  : \"Mosaic cropland (>50%) / natural vegetation (tree, shrub, herbaceous cover) (<50%)\",\n",
    "        40  : \"Mosaic natural vegetation (tree, shrub, herbaceous cover) (>50%) / cropland (<50%)\",\n",
    "        50  : \"Tree cover, broadleaved, evergreen, closed to open (>15%)\",\n",
    "        60  : \"Tree cover, broadleaved, deciduous, closed to open (>15%)\",\n",
    "        70  : \"Tree cover, needleleaved, evergreen, closed to open (>15%)\",\n",
    "        80  : \"Tree cover, needleleaved, deciduous, closed to open (>15%)\",\n",
    "        90  : \"Tree cover, mixed leaf type (broadleaved and needleleaved)\",\n",
    "        100 : \"Mosaic tree and shrub (>50%) / herbaceous cover (<50%)\",\n",
    "        110 : \"Mosaic herbaceous cover (>50%) / tree and shrub (<50%)\",\n",
    "        120 : \"Shrubland\",\n",
    "        130 : \"Grassland\",\n",
    "        140 : \"Lichens and mosses\",\n",
    "        150 : \"Sparse vegetation (tree, shrub, herbaceous cover) (<15%)\",\n",
    "        160 : \"Tree cover, flooded, fresh or brackish water\",\n",
    "        170 : \"Tree cover, flooded, saline water\",\n",
    "        180 : \"Shrub or herbaceous cover, flooded, fresh/saline/brackish water\",\n",
    "        190 : \"Urban areas\",\n",
    "        200 : \"Bare areas\",\n",
    "        210 : \"Water bodies\",\n",
    "        220 : \"Permanent snow and ice\"\n",
    "    }\n",
    ")\n",
    "\n",
    "class Clumping:\n",
    "#> Table 3 of Chen et al. (2005); https://doi.org/10.1016/j.rse.2005.05.003\n",
    "#> The land cover classification does not fully match the FAO Land Cover\n",
    "#> Classification System (LCCS).\n",
    "    \n",
    "    class_codes = [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "    legend = OrderedDict(\n",
    "        {\n",
    "            \"1\"  : \"Tree Cover, broadleaf, evergreen\",\n",
    "            \"2\"  : \"Tree Cover, broadleaf, deciduous, closed\",\n",
    "            \"3\"  : \"Tree Cover, broadleaf, deciduous, open\",\n",
    "            \"4\"  : \"Tree Cover, needleleaf, evergreen\",\n",
    "            \"5\"  : \"Tree Cover, needleleaf, deciduous\",\n",
    "            \"6\"  : \"Tree Cover, mixed leaf type\",\n",
    "            \"7\"  : \"Tree Cover, regularly flooded, fresh water\",\n",
    "            \"8\"  : \"Tree Cover, regularly flooded, saline water\",\n",
    "            \"9\"  : \"Mosaic: Tree Cover / Other natural vegetation\",\n",
    "            \"10\" : \"Tree Cover, burnt\",\n",
    "            \"11\" : \"Shrub Cover, closed-open, evergreen\",\n",
    "            \"12\" : \"Shrub Cover, closed-open, deciduous\",\n",
    "            \"13\" : \"Herbaceous Cover, closed-open\",\n",
    "            \"14\" : \"Sparse herbaceous or sparse shrub cover\",\n",
    "            \"15\" : \"Reg. flooded shrub and/or herbaceous cover\",\n",
    "            \"16\" : \"Cultivated and managed areas\",\n",
    "            \"17\" : \"Mosaic: Cropland / Tree Cover / Natural veg\",\n",
    "            \"18\" : \"Mosaic: Cropland / Shrub and/or grass cover\",\n",
    "            \"19\" : \"Bare Areas\" }\n",
    "    )\n",
    "\n",
    "#> This is the mapping from the LCCS to the classes used in Chen et al.; this\n",
    "#> may require adjustment! E.g. a grassland class seems to be missing.\n",
    "    class_of_LCCS = OrderedDict( # not mapped to: 10; unsure: 18, 19\n",
    "        {\n",
    "            10 : [16],\n",
    "            20 : [15,16],\n",
    "            30 : [17,18],\n",
    "            40 : [9],\n",
    "            50 : [1],\n",
    "            60 : [2,3],\n",
    "            70 : [4],\n",
    "            80 : [5],\n",
    "            90 : [6],\n",
    "            100 : [9],\n",
    "            110 : [9,13],\n",
    "# Chen et al.: \"The 'Shrub' and 'Grassland' classes were retained, as the modeling results for \n",
    "#               broadleaf trees can be extended to these classes [...]\"\n",
    "            120 : [1,11,12],\n",
    "            130 : [1], \n",
    "            140 : [19],\n",
    "            150 : [14],\n",
    "            160 : [7],\n",
    "            170 : [8],\n",
    "            180 : [15],\n",
    "            190 : [19],\n",
    "            200 : [19],\n",
    "            210 : [19],\n",
    "            220 : [19]\n",
    "        }\n",
    "    )\n",
    "# clumping index min, max, and mean from Chen et al.'s table:\n",
    "    ci_min = np.array([0.59,\t0.59,\t0.62,\t0.55,\t0.60,\t0.58,\t0.61,\n",
    "                       0.65,\t0.64,\t0.65,\t0.62,\t0.62,\t0.64,\t0.67,\n",
    "                       0.68,\t0.63,\t0.64,\t0.65,\t0.75])\n",
    "    \n",
    "    ci_max = np.array([0.68,\t0.79,\t0.78,\t0.68,\t0.77,\t0.79,\t0.69,\n",
    "                       0.79,\t0.82,\t0.86,\t0.80,\t0.80,\t0.83,\t0.84,\n",
    "                       0.85,\t0.83,\t0.76,\t0.81,\t0.99])\n",
    "\n",
    "    ci_mean = np.array([0.63,\t0.69,\t0.70,\t0.62,\t0.68,\t0.69,\t0.65,\n",
    "                        0.72,\t0.72,\t0.75,\t0.71,\t0.71,\t0.74,\t0.75,\n",
    "                        0.77,\t0.73,\t0.70,\t0.73,\t0.87])\n",
    "\n",
    "# For the uncertainty, we assume that the distribution is uniform above\n",
    "# and below the mean, respectively, thus making the mean also the\n",
    "# median. We then assume that the reported max and min values are the 97.7%-quantiles \n",
    "# which correspond to 2 sigma of a Gaussian distribution. The average over both sided is taken, to\n",
    "# account for cases where the values are not symmetric. Since no information\n",
    "# about the distribution is available in Chen et al. (2005), the clumping\n",
    "# index uncertainty is modelled as a Gaussian here. \n",
    "    ci_uncertainty = 0.5 * 0.5 * (ci_max - ci_min) # 1-sigma; ((max-mean)+(mean-min)=(max-min)\n",
    "# tendencies from Table 3, not used:\n",
    "    ci_d_NL = np.array([ -0.006 , -0.019,  -0.005,  -0.012, -0.033 , -0.024, np.nan   ,  np.nan   ,  -0.013,  -0.036, -0.020,  -0.016,  -0.016,  -0.019,  -0.026,  -0.018,  -0.011,  -0.018,  -0.032 ])\n",
    "    ci_d_EQ = np.array([ -0.004 , -0.001,  0.007 ,  -0.017, np.nan    , -0.018, -0.002,  -0.006,  0.008 ,  np.nan   , -0.010,  0.009 ,  0.003 ,  0.008 ,  0.004 ,  -0.006,  -0.004,  0.001 ,  -0.03  ])\n",
    "    ci_d_SL = np.array([ 0.024  , 0.021 ,  0.025 ,  0.009 , np.nan    , 0.011 , np.nan   ,  np.nan   ,  np.nan   ,  np.nan   , 0.024 ,  0.022 ,  0.026 ,  0.024 ,  0.024 ,  0.026 ,  0.024 ,  0.026 ,  0.027  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide the confusion matrix, copied from the [C3S LCC v2.1 Product quality assessment report for Sentinel-3 OLCI and SLSTR (PQAR)](https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrices of LCC, from C3S ICDR Land Cover Product Quality\n",
    "# Assessment Report (D5.2.2_PQAR_ICDR_LC_v2.1.x_PRODUCTS_v1.0)\n",
    "cm_2016 = np.array([\n",
    "    [ 121,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0  ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 9,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t3,\t7,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 3,\t0,\t0,\t0,\t199,\t15,\t3,\t0,\t3,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t72,\t1,\t8,\t16,\t0,\t0,\t13,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t3,\t23,\t3,\t0,\t0,\t3,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 2,\t0,\t0,\t0,\t8,\t10,\t3,\t1,\t0,\t0,\t0,\t6,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0  ],\n",
    "    [ 8,\t0,\t0,\t0,\t7,\t19,\t0,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0  ],\n",
    "    [ 8,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t12,\t0,\t0,\t1,\t1,\t25,\t1,\t1  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t25,\t0,\t0,\t0,\t0,\t11,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0  ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t13,\t0,\t0,\t1,\t0,\t61,\t0,\t0  ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0  ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0  ]\n",
    "])\n",
    "\n",
    "cm_2017 = np.array([\n",
    "    [ 121,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 9,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t3,\t7,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t199,\t15,\t3,\t0,\t3,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t72,\t1,\t8,\t16,\t0,\t0,\t13,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t54,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t3,\t23,\t3,\t0,\t0,\t3,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t8,\t10,\t3,\t1,\t0,\t0,\t0,\t6,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t7,\t19,\t0,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 8,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t12,\t0,\t0,\t1,\t1,\t25,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t25,\t0,\t0,\t0,\t0,\t11,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t13,\t0,\t0,\t1,\t0,\t61,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n",
    "\n",
    "cm_2018 = np.array([\n",
    "    [ 119,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 9,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t3,\t7,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t197,\t14,\t3,\t0,\t3,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t71,\t1,\t8,\t16,\t0,\t0,\t13,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t3,\t23,\t3,\t0,\t0,\t2,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t8,\t10,\t3,\t1,\t0,\t0,\t0,\t6,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t7,\t19,\t0,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 8,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t14,\t0,\t0,\t1,\t1,\t25,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t24,\t0,\t0,\t0,\t0,\t12,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t12,\t0,\t0,\t1,\t0,\t60,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n",
    "\n",
    "cm_2019 = np.array([\n",
    "    [ 119,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t2,\t7,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t198,\t14,\t3,\t0,\t2,\t0,\t0,\t3,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t71,\t1,\t8,\t16,\t0,\t0,\t12,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t2,\t23,\t3,\t0,\t0,\t2,\t3,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t8,\t10,\t2,\t1,\t0,\t0,\t0,\t5,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 7,\t0,\t0,\t0,\t7,\t19,\t2,\t1,\t0,\t0,\t0,\t105,\t21,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t14,\t0,\t0,\t1,\t1,\t25,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t23,\t0,\t0,\t0,\t0,\t12,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t12,\t0,\t0,\t1,\t0,\t60,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n",
    "\n",
    "cm_2020 = np.array([\n",
    "    [ 119,\t30,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t5,\t14,\t0,\t2,\t0,\t0,\t1,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t24,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t0,\t0,\t0,\t4,\t1,\t0,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 8,\t1,\t0,\t0,\t0,\t7,\t0,\t0,\t0,\t0,\t0,\t2,\t7,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t197,\t14,\t3,\t0,\t2,\t0,\t0,\t3,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 1,\t0,\t0,\t0,\t6,\t71,\t1,\t8,\t16,\t0,\t0,\t12,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t10,\t3,\t53,\t2,\t16,\t0,\t0,\t2,\t0,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t2,\t23,\t3,\t0,\t0,\t2,\t2,\t0,\t4,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t2,\t1,\t1,\t14,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 3,\t0,\t0,\t0,\t8,\t10,\t2,\t1,\t0,\t0,\t0,\t5,\t5,\t0,\t2,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t2,\t3,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 7,\t0,\t0,\t0,\t7,\t19,\t2,\t1,\t0,\t0,\t0,\t105,\t22,\t0,\t8,\t0,\t0,\t0,\t0,\t1,\t0,\t0 ],\n",
    "    [ 9,\t3,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t19,\t64,\t1,\t14,\t0,\t0,\t1,\t1,\t26,\t1,\t1 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t2,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 5,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t23,\t22,\t3,\t23,\t0,\t0,\t0,\t0,\t12,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t6,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t6,\t0,\t1,\t1,\t0,\t4,\t0,\t0,\t0,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t3,\t0,\t0,\t0 ],\n",
    "    [ 2,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t3,\t0,\t12,\t0,\t0,\t0,\t0,\t59,\t0,\t0 ],\n",
    "    [ 0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t57,\t0 ],\n",
    "    [ 0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0 ]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the entries of the confusion matrices from absolute values into probabilities, we define the following routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def abs_to_prob(confusion_matrix_LCC):\n",
    "    # Convert absolute numbers into probabilities:\n",
    "    confusion_matrix_prob_LCC = np.empty([len(confusion_matrix_LCC),len(confusion_matrix_LCC)])\n",
    "    for irow in range(len(confusion_matrix_LCC)):\n",
    "        rowsum = np.sum( confusion_matrix_LCC[irow][:] )\n",
    "        for icol in range(len(confusion_matrix_LCC[irow])):\n",
    "            if ( rowsum > 0 ):\n",
    "                confusion_matrix_prob_LCC[irow][icol] = ( confusion_matrix_LCC[irow][icol] / rowsum )\n",
    "            else:\n",
    "                confusion_matrix_prob_LCC[irow][icol] = 0.\n",
    "        # debug:print(irow,np.sum(confusion_matrix_prob_LCC[irow]))\n",
    "    return confusion_matrix_prob_LCC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can pre-compute the factors required in the conversion and in the uncertainty propagation, since they are a time-independent function of the LCC. The LAIs of all possible 'Chen'-classes related to one LCC by the confusion matrix and by mapping ambiguity are averaged, weighted by probability, to get one most probable LAI per C3S LCC type:\n",
    "\\begin{equation}\n",
    "  \\textit{LAI}_i(\\textit{LAI}_\\textit{eff},\\textrm{LCC}) = \\frac{1}{n}\\sum_{i=1}^n p_i(\\textrm{LCC}) \\times \n",
    "  \\frac{ \\textit{LAI}_\\textit{eff} } { \\Omega_i(\\textrm{LCC}) },\n",
    "\\end{equation}\n",
    "where $i$ is an index running over all $n$ 'Chen'-classes confused with one C3S LCC.\n",
    "\n",
    "Since the factor $\\textit{LAI}_\\textit{eff}$ is constant in the sum, the sum can be pre-computed into a conversion factor:\n",
    "\\begin{equation}\n",
    "  f\\_LAI\\_cl(LCC) = \\frac{1}{n} \\sum_{i=1}^n \\frac{ p_i(\\textrm{LCC}) } { \\Omega_i(\\textrm{LCC}) }.\n",
    "\\end{equation}\n",
    "With this factor, the conversion becomes\n",
    "\\begin{equation}\n",
    "  \\textit{LAI}_i(\\textit{LAI}_\\textit{eff},\\textrm{LCC}) = f\\_LAI\\_cl(LCC) \\times \\textit{LAI}_\\textit{eff}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For the uncertainty propagation, this means:\n",
    "\\begin{equation}\n",
    "  \\sigma_{\\textit{LAI}}(\\textit{LAI}_\\textit{eff},\\textrm{LCC})^2 = \\left( \\sigma_{\\textit{LAI}_\\textit{eff}} \\times f\\_LAI\\_cl \\right)^2 +   \n",
    "  \\left( \\sigma_{\\Omega(\\textrm{LCC})} \\times \\textit{LAI}_\\textit{eff} \\times\n",
    "  ( -\\frac{1}{n})\\sum_{i=1}^n \\frac{ p_i(\\textrm{LCC}) } { \\Omega_i(\\textrm{LCC})^2 } \\right)^2.\n",
    "\\end{equation}\n",
    "We also pre-compute the LAI-independent part of the factor in the second term of the above equation as\n",
    "\\begin{equation}\n",
    "    f\\_LAI\\_unc2\\_cl(LCC) = \\left( \\sigma_{\\Omega(\\textrm{LCC})} \\times (-\\frac{1}{n})\\sum_{i=1}^n \\frac{ p_i(\\textrm{LCC}) } { \\Omega_i(\\textrm{LCC})^2 } \\right)^2.\n",
    "\\end{equation}\n",
    "In the computation of the uncertainty, we are treating the uncertainty of effective LAI and the uncertainty of the clumping factor as independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LCC_to_ix(LCC):\n",
    "    # convert a Land Cover Class to an array index\n",
    "    return(int)((LCC-10)/10)\n",
    "\n",
    "def vLCC_to_ix(LCC):\n",
    "    # convert a scalar LCC or array of LCC's to an array index\n",
    "    mLCC = np.asarray(LCC)\n",
    "    scalar_input = False\n",
    "    if mLCC.ndim == 0:\n",
    "        mLCC = mLCC[None]  # Makes x 1D\n",
    "        scalar_input = True\n",
    "    if scalar_input:\n",
    "        return np.squeeze((int)((mLCC-10)/10+0.5))\n",
    "    return ((LCC-10)/10).round().astype(dtype=np.int8)\n",
    "    \n",
    "   \n",
    "def pre_compute_factors(confusion_matrix_prob_LCC):\n",
    "    #\n",
    "    # Pre-computation of factors for LAI and uncertainty computation\n",
    "    #\n",
    "    # Accounts for three sources of uncertainty:\n",
    "    # (a) Uncertainty of the effective LAI\n",
    "    # (b) Uncertainty caused by mis-classification of Land Cover Class\n",
    "    # (c) Uncertainty of the clumping factors\n",
    "    # (d) Mapping ambiguity between C3S LCC and Chen et al.'s classes\n",
    "    #\n",
    "    # Intermediate \"true\" LAI uncertainty factor from clumping uncertainty:\n",
    "    f_LAI_unc_cl = np.zeros([len(LCCS_codes),len(Clumping.class_codes)]) \n",
    "    f_LAI_unc2_cl = np.zeros([len(LCCS_codes)])\n",
    "    # Intermediate \"true\" LAI factor:\n",
    "    f_LAI_cl = np.zeros([len(LCCS_codes)])\n",
    "    for iLCC in range(len(LCCS_codes)):\n",
    "        if iLCC != LCC_to_ix(LCCS_codes[iLCC]):\n",
    "            raise \"dask limitation requires functional dependence between LCC code and array index.\"       \n",
    "        # Compute variance caused by potential mis-classification\n",
    "        for icol in range(len(confusion_matrix_prob_LCC[iLCC])):\n",
    "            if ( confusion_matrix_prob_LCC[iLCC][icol] > 0. ):\n",
    "                # Loop over list of mapped clumping classes\n",
    "                class_list = Clumping.class_of_LCCS[LCCS_codes[icol]]\n",
    "                nclasses = len(class_list)\n",
    "                print(\"LCC\",LCCS_codes[iLCC],\"could be LCC\",\n",
    "                      LCCS_codes[icol],\"with probability\",int(1000*confusion_matrix_prob_LCC[iLCC][icol])/1000,\"which maps to Chen et al. class(es)\",\n",
    "                      class_list, \"(\",nclasses,\")\") \n",
    "                if ( nclasses <= 0 ):\n",
    "                    raise Exception(\"unmapped class\")\n",
    "                for ct in class_list: # mapping ambiguity between LCCS and Chen classes, no uncertainty assigned\n",
    "                    iclump = Clumping.class_codes.index(ct)\n",
    "                    # Accumulate LAI factor:\n",
    "                    f_LAI_cl[iLCC] += ( confusion_matrix_prob_LCC[iLCC][icol] /\n",
    "                                      Clumping.ci_mean[iclump] ) / nclasses \n",
    "                    # Accumulate uncertainty factor per clumping class; negative sign from derivative arbitrary (squared away in next step), but\n",
    "                    # would matter if uncertainty sources were treated as correlated (extra terms):\n",
    "                    f_LAI_unc_cl[iLCC,iclump] -=  (\n",
    "                        Clumping.ci_uncertainty[iclump] *\n",
    "                        confusion_matrix_prob_LCC[iLCC][icol] /\n",
    "                        Clumping.ci_mean[iclump]**2 ) /  nclasses\n",
    "        f_LAI_unc2_cl[iLCC] = np.sum( f_LAI_unc_cl[iLCC,:]**2 )\n",
    "    return f_LAI_cl,f_LAI_unc2_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the actual computation, we can then make use of these factors, limiting the number of operations per pixel to the required minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_LAI(LAI_eff, LAI_eff_uncertainty,LCC_type):\n",
    "    #\n",
    "    # returns the \"true\" LAI and its uncertainy after\n",
    "    # \n",
    "    # - conversion from effective to \"true\" LAI using associated clumping factor\n",
    "    # - propagation of uncertainty\n",
    "    #\n",
    "    iLCC = vLCC_to_ix(LCC_type-10)\n",
    "    LAI = f_LAI_cl[iLCC] * LAI_eff\n",
    "    LAI_uncertainty = np.sqrt( ( f_LAI_unc2_cl[iLCC] * LAI_eff**2 ) +\n",
    "                               ( f_LAI_cl[iLCC] * LAI_eff_uncertainty )**2 )\n",
    "    LAI_uncertainty = LAI_uncertainty.rename(LAI_eff_uncertainty.name)\n",
    "    LAI_uncertainty.attrs['units'] = LAI_eff_uncertainty.attrs['units'] # forward unit\n",
    "    return LAI.where(np.isfinite(LAI_eff)),LAI_uncertainty.where(np.isfinite(LAI_eff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing for conversion\n",
    "\n",
    "To actually run the conversion, data needs to be read, the pre-computation steps need to be called, and the output needs to be prepared. Here, we start with the set-up by providing the confusion matrices for all years. They are quite similar, and because there is only a limited number of cases, the sum of them all is thought to give a more robust statistics for the estimation of the confusion probabilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCC 10 could be LCC 10 with probability 0.681 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 10 could be LCC 20 with probability 0.17 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 10 could be LCC 50 with probability 0.011 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 10 could be LCC 60 with probability 0.005 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 10 could be LCC 120 with probability 0.028 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 10 could be LCC 130 with probability 0.079 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 10 could be LCC 150 with probability 0.011 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 10 could be LCC 180 with probability 0.005 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 10 could be LCC 200 with probability 0.005 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 20 could be LCC 10 with probability 0.257 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 20 could be LCC 20 with probability 0.685 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 20 could be LCC 60 with probability 0.057 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 30 could be LCC 10 with probability 0.462 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 30 could be LCC 50 with probability 0.215 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 30 could be LCC 60 with probability 0.053 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 30 could be LCC 120 with probability 0.107 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 30 could be LCC 130 with probability 0.161 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 40 could be LCC 10 with probability 0.307 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 40 could be LCC 20 with probability 0.038 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 40 could be LCC 60 with probability 0.269 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 40 could be LCC 120 with probability 0.1 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 40 could be LCC 130 with probability 0.269 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 40 could be LCC 150 with probability 0.015 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 50 could be LCC 10 with probability 0.013 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 50 could be LCC 50 with probability 0.882 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 50 could be LCC 60 with probability 0.064 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 50 could be LCC 70 with probability 0.013 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 50 could be LCC 90 with probability 0.011 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 50 could be LCC 120 with probability 0.01 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 50 could be LCC 130 with probability 0.004 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 60 could be LCC 10 with probability 0.008 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 60 could be LCC 50 with probability 0.051 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 60 could be LCC 60 with probability 0.615 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 60 could be LCC 70 with probability 0.008 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 60 could be LCC 80 with probability 0.068 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 60 could be LCC 90 with probability 0.137 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 60 could be LCC 120 with probability 0.108 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 70 could be LCC 50 with probability 0.11 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 70 could be LCC 60 with probability 0.033 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 70 could be LCC 70 with probability 0.589 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 70 could be LCC 80 with probability 0.022 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 70 could be LCC 90 with probability 0.177 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 70 could be LCC 120 with probability 0.022 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 70 could be LCC 150 with probability 0.044 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 80 could be LCC 70 with probability 0.068 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 80 could be LCC 80 with probability 0.608 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 80 could be LCC 90 with probability 0.079 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 80 could be LCC 120 with probability 0.063 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 80 could be LCC 130 with probability 0.074 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 80 could be LCC 150 with probability 0.105 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 90 could be LCC 60 with probability 0.111 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 90 could be LCC 70 with probability 0.055 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 90 could be LCC 80 with probability 0.055 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 90 could be LCC 90 with probability 0.777 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 100 could be LCC 10 with probability 0.063 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 100 could be LCC 50 with probability 0.212 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 100 could be LCC 60 with probability 0.265 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 100 could be LCC 70 with probability 0.069 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 100 could be LCC 80 with probability 0.026 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 100 could be LCC 120 with probability 0.148 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 100 could be LCC 130 with probability 0.132 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 100 could be LCC 150 with probability 0.053 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 100 could be LCC 180 with probability 0.026 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 110 could be LCC 70 with probability 0.096 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 110 could be LCC 120 with probability 0.258 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 110 could be LCC 130 with probability 0.483 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 110 could be LCC 180 with probability 0.161 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 120 could be LCC 10 with probability 0.044 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 120 could be LCC 50 with probability 0.041 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 120 could be LCC 60 with probability 0.111 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 120 could be LCC 70 with probability 0.004 which maps to Chen et al. class(es) [4] ( 1 )\n",
      "LCC 120 could be LCC 80 with probability 0.005 which maps to Chen et al. class(es) [5] ( 1 )\n",
      "LCC 120 could be LCC 120 with probability 0.615 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 120 could be LCC 130 with probability 0.124 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 120 could be LCC 150 with probability 0.046 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 120 could be LCC 200 with probability 0.005 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 10 with probability 0.06 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 130 could be LCC 20 with probability 0.021 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 130 could be LCC 90 with probability 0.007 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 130 could be LCC 120 with probability 0.136 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 130 could be LCC 130 with probability 0.461 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 130 could be LCC 140 with probability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 150 with probability 0.095 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 130 could be LCC 180 with probability 0.007 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 130 could be LCC 190 with probability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 200 with probability 0.181 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 210 with probability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 130 could be LCC 220 with probability 0.007 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 140 could be LCC 140 with probability 0.666 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 140 could be LCC 150 with probability 0.333 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 150 could be LCC 10 with probability 0.056 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 150 could be LCC 120 with probability 0.259 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 150 could be LCC 130 with probability 0.248 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 150 could be LCC 140 with probability 0.033 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 150 could be LCC 150 with probability 0.27 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 150 could be LCC 200 with probability 0.13 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 160 could be LCC 50 with probability 1.0 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 170 could be LCC 180 with probability 1.0 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 180 could be LCC 130 with probability 0.5 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 180 could be LCC 150 with probability 0.083 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 180 could be LCC 160 with probability 0.083 which maps to Chen et al. class(es) [7] ( 1 )\n",
      "LCC 180 could be LCC 180 with probability 0.333 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 190 could be LCC 60 with probability 0.25 which maps to Chen et al. class(es) [2, 3] ( 2 )\n",
      "LCC 190 could be LCC 190 with probability 0.75 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 200 could be LCC 10 with probability 0.025 which maps to Chen et al. class(es) [16] ( 1 )\n",
      "LCC 200 could be LCC 120 with probability 0.012 which maps to Chen et al. class(es) [1, 11, 12] ( 3 )\n",
      "LCC 200 could be LCC 130 with probability 0.037 which maps to Chen et al. class(es) [1] ( 1 )\n",
      "LCC 200 could be LCC 150 with probability 0.156 which maps to Chen et al. class(es) [14] ( 1 )\n",
      "LCC 200 could be LCC 180 with probability 0.01 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 200 could be LCC 200 with probability 0.758 which maps to Chen et al. class(es) [19] ( 1 )\n",
      "LCC 210 could be LCC 20 with probability 0.016 which maps to Chen et al. class(es) [15, 16] ( 2 )\n",
      "LCC 210 could be LCC 90 with probability 0.016 which maps to Chen et al. class(es) [6] ( 1 )\n",
      "LCC 210 could be LCC 180 with probability 0.003 which maps to Chen et al. class(es) [15] ( 1 )\n",
      "LCC 210 could be LCC 210 with probability 0.962 which maps to Chen et al. class(es) [19] ( 1 )\n"
     ]
    }
   ],
   "source": [
    "# Set-up\n",
    "confusion_matrix_prob_LCC = abs_to_prob(cm_2016+cm_2017+cm_2018+cm_2019+cm_2020)\n",
    "f_LAI_cl,f_LAI_unc2_cl = pre_compute_factors(confusion_matrix_prob_LCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: something to play around with\n",
    "\n",
    "Feel free to vary the inputs in the following cell, as long as all arrays have the same length. This should give you an idea of the magnitude of change of LAI and its uncertainties caused by the conversion for different LCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCC type <xarray.DataArray 'LCCS_type' (dim_0: 6)> Size: 48B\n",
      "array([160, 160, 160, 120, 120, 120])\n",
      "Dimensions without coordinates: dim_0\n",
      "<xarray.DataArray 'LCCS_type' ()> Size: 8B\n",
      "array(160)  :  Tree cover, flooded, fresh or brackish water\n",
      "<xarray.DataArray 'LCCS_type' ()> Size: 8B\n",
      "array(160)  :  Tree cover, flooded, fresh or brackish water\n",
      "<xarray.DataArray 'LCCS_type' ()> Size: 8B\n",
      "array(160)  :  Tree cover, flooded, fresh or brackish water\n",
      "<xarray.DataArray 'LCCS_type' ()> Size: 8B\n",
      "array(120)  :  Shrubland\n",
      "<xarray.DataArray 'LCCS_type' ()> Size: 8B\n",
      "array(120)  :  Shrubland\n",
      "<xarray.DataArray 'LCCS_type' ()> Size: 8B\n",
      "array(120)  :  Shrubland\n",
      "LAI_eff : <xarray.DataArray 'LAI' (dim_0: 6)> Size: 48B\n",
      "array([1., 2., 3., 1., 2., 3.])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2 \tLAI_eff_unc :  <xarray.DataArray 'LAI_ERR' (dim_0: 6)> Size: 48B\n",
      "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n",
      "LAI     : <xarray.DataArray 'LAI' (dim_0: 6)> Size: 48B\n",
      "array([1.40312771, 2.80625543, 4.20938314, 1.51246069, 3.02492137,\n",
      "       4.53738206])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2 \tLAI_unc     :  <xarray.DataArray 'LAI_ERR' (dim_0: 6)> Size: 48B\n",
      "array([0.28257368, 0.28833913, 0.29770019, 0.30473558, 0.31136893,\n",
      "       0.3221211 ])\n",
      "Dimensions without coordinates: dim_0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "LAI = np.empty([6])\n",
    "LAI_uncertainty = np.empty([6])\n",
    "# Input\n",
    "LAI_eff = xr.DataArray([1.,2.,3.,1.,2.,3.],name='LAI',attrs={'units':'m2.m-2'})\n",
    "LAI_eff_uncertainty = xr.DataArray([0.2,0.2,0.2,0.2,0.2,0.2],name='LAI_ERR',attrs={'units':'m2.m-2'})\n",
    "LCC_type = xr.DataArray([160,160,160, 120, 120, 120],name='LCCS_type')\n",
    "# Actual conversion:\n",
    "LAI, LAI_uncertainty = convert_LAI(LAI_eff, LAI_eff_uncertainty, LCC_type)\n",
    "print(\"LCC type\",LCC_type)\n",
    "for cLCC in LCC_type:\n",
    "    print(cLCC,\" : \",LCCS_legend[(int)(cLCC.compute())])\n",
    "print(\"LAI_eff :\",LAI_eff,\"\\tLAI_eff_unc : \",LAI_eff_uncertainty)\n",
    "print(\"LAI     :\",LAI    ,\"\\tLAI_unc     : \",LAI_uncertainty    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "request"
    ]
   },
   "source": [
    "### Select the data\n",
    "\n",
    "We will use the [effective LAI, flags, and uncertainty](https://cds.climate.copernicus.eu/datasets/satellite-lai-fapar?tab=overview) and [C3S LCC](https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=overview) data from the CDS.\n",
    "\n",
    "Having selected the correct dataset, we now need to specify what parameters we are interested in. These parameters can all be selected in the **\"Download data\"** tab. In this tab a form appears in which we will select the following parameters to download:\n",
    "\n",
    ":::{dropdown} Parameters of data to download\n",
    "\n",
    "- Format: `tgz`\n",
    "- Variable: `LAI`\n",
    "- Satellite: `Sentinel 3`\n",
    "- Sensor: `OLCI and SLSTR`\n",
    "- Horizontal resolution: `300m`\n",
    "- Product version: `v4`\n",
    "- Year: `2019`\n",
    "- Month: `04`\n",
    "- Nominal day: `10`\n",
    "- Geographical area: `60, 0, 40, 20`\n",
    "\n",
    ":::\n",
    "\n",
    "At the end of the download form, select **\"Show API request\"**. This will reveal a block of code, which you can simply copy and paste into a cell of your Jupyter Notebook (see cell below). Having copied the API request into the cell below, running this will retrieve and download the data you requested into your local directory.\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "Please remember to accept the terms and conditions of the dataset, at the bottom of the CDS download form!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "request"
    ]
   },
   "source": [
    "### Download the data\n",
    "\n",
    "Our code defines a python subroutine `get_data` which downloads LAI and fAPAR data as gzip'ped `tar` archive from the CDS and writes it to the file given by the `target` argument of `get_data`.\n",
    "The data is retained in a file to avoid repeated downloading when the notebook is run repeatedly and to avoid holding the whole data in memory. This comes at the cost of dublicating the data on disk in the subsequent extraction step. Currently, there seems to be no way to avoid this, because the CDS *always* delivers compressed archives when the data is ordered as file.\n",
    "\n",
    "For this tutorial, we are downloading global effective LAI V4 based on Sentinel-3 (OLCI and SLSTR) data with a spatial resolution of 300 m for April 10, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "request"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:00:56,958 INFO Request ID is 72743fa2-bc86-4162-a321-632aaf3b4b4c\n",
      "2025-10-16 11:00:57,019 INFO status has been updated to accepted\n",
      "2025-10-16 11:01:09,625 INFO status has been updated to running\n",
      "Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Read timed out. (read timeout=60)], attemps 1 of 500\n",
      "Retrying in 120 seconds\n",
      "2025-10-16 11:10:15,036 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1119d1e593cf60eab86150c01c92b91d.gz:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got LAI data,       elapsed: 0:09:31.086275\n"
     ]
    }
   ],
   "source": [
    "def get_data(dataset, request, target):\n",
    "    import cdsapi\n",
    "    import os.path\n",
    "    if os.path.isfile(target):\n",
    "        print(\"target\",target,\"already exists.\")\n",
    "    else:\n",
    "        client = cdsapi.Client(key = cdsapi_key, url = cdsapi_url)\n",
    "        client.retrieve(dataset,request,target) #.download()\n",
    "            \n",
    "starttime = datetime.now()\n",
    "dataset = 'satellite-lai-fapar'\n",
    "request = {\n",
    "    'format': 'tgz',\n",
    "    'variable': ['lai'],\n",
    "    'satellite': ['sentinel_3'],\n",
    "    'sensor': 'olci_and_slstr',\n",
    "    'horizontal_resolution': ['300m'],\n",
    "    'product_version': 'v4',\n",
    "    'year': ['2019'],\n",
    "    'month': ['04'],\n",
    "    'nominal_day': ['10'],\n",
    "    'area': [60, 0, 40, 20]\n",
    "}\n",
    "\n",
    "laifile = 'laidata.tgz'\n",
    "get_data(dataset,request,target=laifile)\n",
    "print('got LAI data,       elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar step is required for the LCC data, to get the corresponding land cover classes for 2019. The resolution of this product is 300 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "request"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:10:27,862 INFO [2025-07-04T00:00:00] Due to a transition between project phases, there are changes to the timeline of this dataset updates, which are usually on an annual basis with a one year delay: 2023 and 2024 data updates are now expected during 2026. Please watch the [forum](https://forum.ecmwf.int/c/announcements/5) for future announcements.\n",
      "2025-10-16 11:10:27,863 INFO Request ID is 6c796844-237c-4761-a9e1-bc3b411b37e7\n",
      "2025-10-16 11:10:27,958 INFO status has been updated to accepted\n",
      "2025-10-16 11:10:40,621 INFO status has been updated to running\n",
      "2025-10-16 11:16:49,239 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1326539343079d75684eaf77cb9bb2f6.gz:   0%|          | 0.00/51.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got LCC data,       elapsed: 0:06:27.691644\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "dataset = \"satellite-land-cover\"\n",
    "request = {\n",
    "    'format': 'tgz',\n",
    "    'variable': 'all',\n",
    "    'year': ['2019'],\n",
    "    'version': ['v2_1_1'],\n",
    "    'area': [60, 0, 40, 20]\n",
    "}\n",
    "lccfile = 'lccdata.tgz'\n",
    "get_data(dataset,request,target=lccfile)\n",
    "print('got LCC data,       elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Unpack the data\n",
    "\n",
    "The .tgz file used in the transfer from the CDS is like a tightly packed box of information (a compressed archive). To use the data inside, we first need to unpack and decompress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened tar file,  elapsed: 0:00:00.001108\n",
      "?rw-r--r-- root/root  111877524 2025-10-16 11:06:11 c3s_LAI_20190410000000_GLOBE_SENTINEL3_V4.0.1.area-subset.60.20.40.0.nc \n",
      "listing,     elapsed: 0:00:00.255625\n",
      "extracting  c3s_LAI_20190410000000_GLOBE_SENTINEL3_V4.0.1.area-subset.60.20.40.0.nc\n",
      "opened tar file,  elapsed: 0:00:00.653863\n",
      "?rw-r--r-- root/root   54252498 2025-10-16 11:14:50 C3S-LC-L4-LCCS-Map-300m-P1Y-2019-v2.1.1.area-subset.60.20.40.0.nc \n",
      "listing,     elapsed: 0:00:00.769791\n",
      "extracting  C3S-LC-L4-LCCS-Map-300m-P1Y-2019-v2.1.1.area-subset.60.20.40.0.nc\n",
      "unpacked data,    elapsed: 0:00:00.961895\n"
     ]
    }
   ],
   "source": [
    "def unpack_data(file):\n",
    "    import tarfile\n",
    "    import os.path\n",
    "    tf = tarfile.open(name=file,mode='r')\n",
    "    print('opened tar file,  elapsed:',datetime.now()-starttime)\n",
    "    tf.list()\n",
    "    print('listing,     elapsed:',datetime.now()-starttime)\n",
    "    # just extract what is not present:\n",
    "    for xfile in tf:\n",
    "        if os.path.isfile(xfile.name) == False:\n",
    "            print('extracting ',xfile.name)\n",
    "            tf.extract(member=xfile.name,path='.') # uses current working directory        \n",
    "        else:\n",
    "            print('present    ',xfile.name)\n",
    "    return tf\n",
    "\n",
    "starttime = datetime.now()\n",
    "lai_tarfileinfo = unpack_data(laifile)\n",
    "lcc_tarfileinfo = unpack_data(lccfile)\n",
    "print('unpacked data,    elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Inspect the data\n",
    "The data are prepared for reading by passing their names to `xarray` file objects called `laifiledata` and `lccfiledata` here. The commented-out code lines are alternatives that may be used when the computations run with `dask` -- which is currently not the case because of the limitations in section '(Install and) Import libraries' above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up inout files: 0:00:04.053962\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "varname = 'LAI' \n",
    "uncname = varname + '_ERR' # name of uncertainty layer\n",
    "# Extract the file names containting `varname` from `tarfileinfo`\n",
    "inputfiles = [] # start with empty list\n",
    "for xfile in lai_tarfileinfo:\n",
    "    if varname.casefold() in xfile.name.casefold():\n",
    "        inputfiles.append(xfile.name)\n",
    "# For dask: Give the list to an `xarray` multi-file object\n",
    "#dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "#laifiledata = xr.open_mfdataset(inputfiles,chunks='auto',parallel=True)\n",
    "#laifiledata = xr.open_dataset(inputfiles[0],chunks='auto')\n",
    "# Without dask:\n",
    "laifiledata = xr.load_dataset(inputfiles[0])\n",
    "lccname = 'lccs_class'\n",
    "inputfiles = [] # start with empty list\n",
    "for xfile in lcc_tarfileinfo:\n",
    "    if 'LCCS-Map'.casefold() in xfile.name.casefold():\n",
    "        inputfiles.append(xfile.name)\n",
    "#lccfiledata = xr.open_mfdataset(inputfiles,chunks='auto',parallel=True)\n",
    "#lccfiledata = xr.open_dataset(inputfiles[0],chunks='auto')\n",
    "lccfiledata = xr.load_dataset(inputfiles[0])\n",
    "\n",
    "#\n",
    "print('set up inout files:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Apply quality flags\n",
    "TIP LAI and fAPAR come with a set of flags containing information about the retrieval and its quality. They are stored as individual bits in the layer `retrieval_flag`. We are using the hexadecimal representation `0x1C1` of the bit array `111000001`, here, to avoid cells with the conditions `obs_is_fillvalue`, `tip_untrusted`,`obs_unusable`, and `obs_inconsistent` (see [v4.0 Product user guide](https://cds.climate.copernicus.eu/datasets/satellite-lai-fapar?tab=documentation) for reference). In the end, we must not forget to define the units of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applied flags,  elapsed: 0:00:00.583084\n"
     ]
    }
   ],
   "source": [
    "def apply_flags(data,fielddict):\n",
    "    import numpy as np\n",
    "    func     = lambda val, flags : np.where( (np.bitwise_and(flags.astype('uint32'),0x1C1) == 0x0 ), val, np.nan )\n",
    "    units = data[fielddict['variable']].attrs['units']\n",
    "    clean_data = xr.apply_ufunc(func,\\\n",
    "                                data[fielddict['variable']],\\\n",
    "                                data[fielddict['flags']],\\\n",
    "                                dask=\"allowed\",dask_gufunc_kwargs={'allow_rechunk':True})\n",
    "    # Set units of result:\n",
    "    clean_data.attrs['units'] = units\n",
    "    return clean_data\n",
    "\n",
    "starttime = datetime.now()\n",
    "laifiledata[varname] = apply_flags(laifiledata,{'variable':varname,'flags':'retrieval_flag'})\n",
    "laifiledata[uncname] = apply_flags(laifiledata,{'variable':uncname,'flags':'retrieval_flag'})\n",
    "print('applied flags,  elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Run the conversion\n",
    "Now that the data and set up are ready, it is time to run the conversion. Note that with these data, it turns out that the datasets to be combined are not defined on the same grid, even if both use a global regular grid with a spatial resolution of approximately 300 m at the equator. Therefore, a nearest neighbour interpolation is nested into the call of `conversion_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'LAI' (time: 1, lat: 6720, lon: 6720)> Size: 181MB\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [0.75202197, 0.64169085, 0.6520677 , ..., 0.8435831 ,\n",
      "         0.9189684 , 0.55409735],\n",
      "        [0.6748054 , 0.61712193,        nan, ..., 0.8974515 ,\n",
      "         0.8507554 , 0.916069  ],\n",
      "        [0.6218526 , 0.58675414, 0.6010987 , ..., 0.8415993 ,\n",
      "         0.55287653, 0.61162823]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2019-04-10\n",
      "  * lon      (lon) float64 54kB 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 54kB 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n",
      "<xarray.DataArray 'LAI_ERR' (time: 1, lat: 6720, lon: 6720)> Size: 181MB\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [0.61208606, 0.5263238 , 0.5374637 , ..., 0.84816116,\n",
      "         0.9334656 , 0.5267816 ],\n",
      "        [0.5716466 , 0.528155  ,        nan, ..., 0.9446055 ,\n",
      "         0.7863574 , 0.8448039 ],\n",
      "        [0.55272394, 0.51960933, 0.5835495 , ..., 0.7639249 ,\n",
      "         0.49641386, 0.441935  ]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2019-04-10\n",
      "  * lon      (lon) float64 54kB 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 54kB 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2\n",
      "<xarray.DataArray 'lccs_class' (time: 1, lat: 7200, lon: 7200)> Size: 52MB\n",
      "array([[[210, 210, 210, ..., 210, 210, 210],\n",
      "        [210, 210, 210, ..., 210, 210, 210],\n",
      "        [210, 210, 210, ..., 210, 210, 210],\n",
      "        ...,\n",
      "        [ 12,  12,  12, ...,  12,  40,  40],\n",
      "        [ 12,  12,  12, ...,  12,  12,  30],\n",
      "        [ 12,  12,  12, ...,  12,  12,  12]]], dtype=uint8)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 58kB 60.0 60.0 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "  * lon      (lon) float64 58kB 0.001389 0.004167 0.006944 ... 19.99 20.0 20.0\n",
      "  * time     (time) datetime64[ns] 8B 2019-04-10\n",
      "Attributes:\n",
      "    standard_name:        land_cover_lccs\n",
      "    flag_colors:          #ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #00...\n",
      "    long_name:            Land cover class defined in LCCS\n",
      "    valid_min:            1\n",
      "    valid_max:            220\n",
      "    ancillary_variables:  processed_flag current_pixel_state observation_coun...\n",
      "    flag_meanings:        no_data cropland_rainfed cropland_rainfed_herbaceou...\n",
      "    flag_values:          [  0  10  11  12  20  30  40  50  60  61  62  70  7...\n",
      "<xarray.DataArray 'lccs_class' (time: 1, lat: 6720, lon: 6720)> Size: 361MB\n",
      "array([[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        [ nan, 210., 210., ..., 210., 210., 210.],\n",
      "        [ nan, 210., 210., ..., 210., 210.,  70.],\n",
      "        ...,\n",
      "        [ nan,  12.,  12., ...,  11.,  12.,  40.],\n",
      "        [ nan,  12.,  12., ...,  10.,  12.,  12.],\n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan]]])\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2019-04-10\n",
      "  * lon      (lon) float64 54kB 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 54kB 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    standard_name:        land_cover_lccs\n",
      "    flag_colors:          #ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #00...\n",
      "    long_name:            Land cover class defined in LCCS\n",
      "    valid_min:            1\n",
      "    valid_max:            220\n",
      "    ancillary_variables:  processed_flag current_pixel_state observation_coun...\n",
      "    flag_meanings:        no_data cropland_rainfed cropland_rainfed_herbaceou...\n",
      "    flag_values:          [  0  10  11  12  20  30  40  50  60  61  62  70  7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/.local/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:237: RuntimeWarning: invalid value encountered in cast\n",
      "  return data.astype(dtype, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up computation,  elapsed: 0:00:07.431187\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "# Test output to check objects\n",
    "print (laifiledata[varname])\n",
    "print (laifiledata[uncname])\n",
    "lccfiledata['time'] = laifiledata['time'] # to align time dimension\n",
    "print(lccfiledata[lccname])\n",
    "print(lccfiledata[lccname].\\\n",
    "                         sel(lat=slice(laifiledata.lat.max(),laifiledata.lat.min())).\\\n",
    "                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=False))\n",
    "# Define conversion as lambda function to apply in dask delayed computation\n",
    "conversion_func = lambda var, unc, lcc : convert_LAI(LAI_eff=var, LAI_eff_uncertainty=unc, LCC_type=lcc)\n",
    "# Set up the conversion\n",
    "#LAI  = dask.array.apply_gufunc(conversion_func,\n",
    "#                      '(i,j),(i,j),(i,j)->(i,j)',\n",
    "#                      laifiledata[varname],\n",
    "#                      laifiledata[uncname],\n",
    "#                      lccfiledata[lccname].\\\n",
    "#                         sel(lat=slice(laifiledata.lat.min(),laifiledata.lat.max())).\\\n",
    "#                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=True)\n",
    "#                      )\n",
    "#LAI, LAIu  = xr.apply_ufunc(conversion_func,\n",
    "#                      laifiledata[varname],\n",
    "#                      laifiledata[uncname],\n",
    "#                      lccfiledata[lccname].\\\n",
    "#                         sel(lat=slice(laifiledata.lat.max(),laifiledata.lat.min())).\\\n",
    "#                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=False),\n",
    "#                      dask=\"allowed\")\n",
    "LAI = convert_LAI(LAI_eff = laifiledata[varname],\n",
    "                   LAI_eff_uncertainty = laifiledata[uncname],\n",
    "                   LCC_type = lccfiledata[lccname].\\\n",
    "                         sel(lat=slice(laifiledata.lat.max(),laifiledata.lat.min())).\\\n",
    "                         interp(coords={'lon':laifiledata.lon,'lat':laifiledata.lat},method='nearest',assume_sorted=False))\n",
    "\n",
    "print('set up computation,  elapsed:',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There may be an \"*RuntimeWarning: invalid value encountered in cast*\" caused by the missing values in the input data, which seems safe to be ignored.\n",
    "\n",
    "With dask, the computation could actually be delayed until the data values are used, and large datasets could be processed in parallel. Here, we run an output method, which would start the sliced computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<xarray.DataArray 'LAI' (time: 1, lat: 6720, lon: 6720)> Size: 361MB\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [1.04131915, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.8122886 ],\n",
      "        [0.93439795, 0.        ,        nan, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.86107457, 0.81247404, 0.83233686, ..., 1.16535618,\n",
      "         0.76556396, 0.846917  ]]])\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2019-04-10\n",
      "  * lon      (lon) float64 54kB 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 54kB 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2, <xarray.DataArray 'LAI_ERR' (time: 1, lat: 6720, lon: 6720)> Size: 361MB\n",
      "array([[[       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [0.84930186, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.77273374],\n",
      "        [0.79306436, 0.        ,        nan, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.76667871, 0.72075493, 0.80921031, ..., 1.05955842,\n",
      "         0.68854762, 0.61354773]]])\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2019-04-10\n",
      "  * lon      (lon) float64 54kB 8.185e-10 0.002976 0.005952 ... 19.99 19.99 20.0\n",
      "  * lat      (lat) float64 54kB 60.0 59.99 59.99 59.99 ... 40.01 40.01 40.0 40.0\n",
      "Attributes:\n",
      "    units:    m2.m-2)\n",
      "Writing to  LAI-clumped.nc\n",
      "After writing output : 0:00:14.976739\n"
     ]
    }
   ],
   "source": [
    "print(LAI)\n",
    "#\n",
    "# Output to file triggers the delayed computation:\n",
    "#\n",
    "def write_result(output,outfile):\n",
    "    import os.path\n",
    "    if os.path.isfile(outfile):\n",
    "        print(\"file\",outfile,\"already exists. Writing skipped.\")\n",
    "    else:\n",
    "        print(\"Writing to \",outfile)\n",
    "        output.to_netcdf(outfile,mode='w',encoding={varname:{\"zlib\": True, \"complevel\": 4,},uncname:{\"zlib\": True, \"complevel\": 4}})\n",
    "    return\n",
    "\n",
    "starttime = datetime.now()\n",
    "output = xr.merge(LAI)\n",
    "outfile = varname + '-clumped.nc'\n",
    "write_result(output,outfile)\n",
    "output.close()\n",
    "print('After writing output :',datetime.now()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Output data\n",
    "The notebook leaves the following files behind:\n",
    "- *laidata.tgz* - compressed tar archive retrieved from CDS\n",
    "- *lccdata.tgz* - compressed tar archive retrieved from CDS\n",
    "- *c3s_LAI_20190410000000_GLOBE_SENTINEL3_V4.0.1.area-subset.60.0.40.20.nc* - input LAI data\n",
    "- *C3S-LC-L4-LCCS-Map-300m-P1Y-2019-v2.1.1.area-subset.60.0.40.20.nc* - input LCC data\n",
    "- *LAI-clumped.nc* - output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "key-messages"
    ]
   },
   "source": [
    "## Take home messages ðŸ“Œ\n",
    "\n",
    "- The Climate Data Store contains many useful LAI and LAI-related datasets.\n",
    "- The distinction between effective and true LAI is very important -- conversion may be needed to ensure accurate representation of vegetation properties for your use case.\n",
    "- Successfully converting LAI data requires careful preparation and an understanding of the input data structure, formats and the uncertainties introduced by the process.\n",
    "- LAI conversions are complex. While they appear trivial, they involve assumptions that can impact results, so itâ€™s important to interpret outputs cautiously and be aware of potential uncertainties."
   ]
  }
 ],
 "metadata": {
  "cells": [
   {
    "cell_type": "",
    "execution_count": null,
    "id": "",
    "metadata": {},
    "outputs": [],
    "source": []
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  },
  "metadata": {
   "author": "",
   "content_type": "Software & code",
   "data_access": "Climate Data Store",
   "deployment": {
    "ecmwf": {
     "deployment_service": {
      "link": "",
      "service_contact": "",
      "service_provider": ""
     },
     "git": {
      "link": "https://github.com/ecmwf-training/c3s-training-submodule-sat-obs-land/blob/develop/convert-effective-lai-to-lai.ipynb",
      "service_contact": "copernicus-training@ecmwf.int",
      "service_provider": "ECMWF C3S"
     }
    },
    "wekeo": {
     "git": {
      "link": "",
      "service_contact": "copernicus-training@ecmwf.int",
      "service_provider": "ECMWF C3S"
     },
     "url": {
      "link": "",
      "service_contact": "copernicus-training@ecmwf.int",
      "service_provider": "ECMWF C3S"
     }
    }
   },
   "description": "A practical usecase on how to estimate effective Leaf Area Index(LAI) from Leaf Area Index using a clumping parameter. It also further extends conversion of effective LAI to true LAI.",
   "image": "",
   "license": "Apache-2.0",
   "metadata_schema_version": "2.0.0",
   "originator": "C3S",
   "tags": {
    "category": "c3s",
    "data_product": "Leaf Area Index (LAI)",
    "data_provider": "Copernicus C3S",
    "data_type": "observation",
    "orbit": "",
    "satellite": "",
    "sensor": "",
    "service": "",
    "subtheme": [
     "vegetation"
    ],
    "theme": "Surface features",
    "variable": [
     "Leaf Area Index",
     "Effective Leaf Area Index",
     "True Leaf Area Index"
    ]
   },
   "title": "Convert effective LAI to LAI",
   "version": "1.0.0",
   "version_date": ""
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
